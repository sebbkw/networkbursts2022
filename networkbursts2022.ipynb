{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import all the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import skfuzzy as fuzz\n",
    "\n",
    "from open_ephys.analysis import Session\n",
    "\n",
    "from spectrum import dpss, pmtm\n",
    "\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.stats.descriptivestats import sign_test\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "from scipy import signal\n",
    "from scipy import interpolate\n",
    "from scipy import cluster\n",
    "from scipy import io\n",
    "from scipy import ndimage\n",
    "from scipy import stats\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes, utility functions and constants used throughout notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########\n",
    "# Classes #\n",
    "###########\n",
    "\n",
    "class Burst:\n",
    "    def __init__ (self, time, data):\n",
    "        self.time = time\n",
    "        self.data = data\n",
    "        \n",
    "    def get_xticks (self):\n",
    "        return get_xticks(slice(*self.time))\n",
    "\n",
    "############################\n",
    "# Graph plotting functions #\n",
    "############################\n",
    "\n",
    "def format_plot (ax, legend=True, size=[12.5, 15]): \n",
    "    if legend:\n",
    "        leg = plt.legend(frameon=False, bbox_to_anchor=(1, 1), loc='upper left')\n",
    "        for legobj in leg.legendHandles:\n",
    "            legobj.set_linewidth(2.0)\n",
    "            legobj.set_alpha(1)\n",
    "        for i in ax.get_legend().get_texts():\n",
    "            i.set_fontsize(size[1])\n",
    "    \n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    \n",
    "    for item in ([ax.title] + ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "        item.set_fontsize(size[0])\n",
    "    for item in ([ax.xaxis.label, ax.yaxis.label]):\n",
    "        item.set_fontsize(size[1])\n",
    "\n",
    "def plot_graph_by_age (data, ylabel, scatter, colors, labels, save=False):\n",
    "    for brain_idx, brain_area in enumerate(data.keys()):\n",
    "        data_by_age = data[brain_area]\n",
    "        xpos = np.arange(len(data_by_age.keys()))\n",
    "        xlabels = [age for age in data_by_age.keys()]\n",
    "        \n",
    "        mean = [np.mean(d) for d in data_by_age.values()]\n",
    "        stderr = [np.std(d)/len(d)**0.5 for d in data_by_age.values()]\n",
    "\n",
    "        if scatter:\n",
    "            xpos_scatter, value_scatter = [], []\n",
    "            for idx, d in enumerate(data_by_age.values()):\n",
    "                for val in d:\n",
    "                    xpos_scatter.append(idx)\n",
    "                    value_scatter.append(val)\n",
    "            plt.plot(xpos_scatter, value_scatter, c=colors[brain_idx], lw=0, marker='o', alpha=0.25)\n",
    "\n",
    "        plt.errorbar(xpos, mean, yerr=stderr, capsize=5, c=colors[brain_idx], label=labels[brain_idx])\n",
    "        plt.xticks(xpos, xlabels)\n",
    "        plt.xlabel('Age (days)')\n",
    "        plt.ylabel(ylabel)\n",
    "        \n",
    "    ax = plt.gca()\n",
    "        \n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    handles = [h[0] for h in handles]\n",
    "    leg = ax.legend(handles, labels, frameon=False, bbox_to_anchor=(1, 1), loc='upper left')\n",
    "\n",
    "    for legobj in leg.legendHandles:\n",
    "        legobj.set_linewidth(2.0)\n",
    "    \n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    \n",
    "    for item in ([ax.title] + ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "        item.set_fontsize(12.5)\n",
    "    for item in ([ax.xaxis.label, ax.yaxis.label] + ax.get_legend().get_texts()):\n",
    "        item.set_fontsize(15)\n",
    "    \n",
    "    if save:\n",
    "        plt.savefig(FIG_ROOT + save, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    \n",
    "def plot_threshold (ax, data_range, threshold):\n",
    "    x = get_xticks(data_range)\n",
    "    y = np.ones(len(x)) * threshold\n",
    "    \n",
    "    ax.plot(x, y)\n",
    "    ax.plot(x, -y)\n",
    "    \n",
    "def plot_frequency_bands (ax):\n",
    "    ax.axvline(x=4, ymin=0, ymax=1, c='orange') # Alpha\n",
    "    ax.axvline(x=8, ymin=0, ymax=1, c='orange') # Theta\n",
    "    ax.axvline(x=13, ymin=0, ymax=1, c='orange') # Beta\n",
    "    ax.axvline(x=30, ymin=0, ymax=1, c='orange') # Gamma\n",
    "    ax.axvline(x=80, ymin=0, ymax=1, c='orange')\n",
    "    \n",
    "def plot_spectrogram (burst, ax=None):    \n",
    "    freq_lim = 50\n",
    "    window_size = 0.5\n",
    "    \n",
    "    window = int(SAMPLING_RATE*window_size)\n",
    "    overlap = int(SAMPLING_RATE*window_size*0.99)\n",
    "    \n",
    "    f, t, Sxx = signal.spectrogram(burst.data, SAMPLING_RATE, nperseg=window, noverlap=overlap)\n",
    "    \n",
    "    f_lim = np.where(f <= freq_lim+25)[0][-1]\n",
    "    f = f[:f_lim]\n",
    "    Sxx = Sxx[:f_lim, :]\n",
    "    \n",
    "    t += burst.time[0]/SAMPLING_RATE\n",
    "    \n",
    "    if ax == None:\n",
    "        fig, ax = plt.subplots()\n",
    "    \n",
    "    ax.pcolormesh(t, f, ndimage.gaussian_filter(Sxx, sigma=0), shading='gouraud', vmax=np.percentile(Sxx.flatten(), 98))\n",
    "    ax.set_ylim([0, freq_lim])\n",
    "    ax.set_yticks(np.arange(0, freq_lim, 20))    \n",
    "    \n",
    "def plot_psd (data, scale=False):\n",
    "    f, Pxx = multitaper_psd(data)\n",
    "    \n",
    "    # Find 0-100 Hz range\n",
    "    max_idx = np.where(f <= 100)[0][-1]\n",
    "    f = f[:max_idx]\n",
    "    Pxx = Pxx[:max_idx]\n",
    "    \n",
    "    # Scale total range to 1\n",
    "    if scale:\n",
    "        Pxx = scale_0_to_1 (Pxx)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=[10,2])\n",
    "    plt.plot(f, Pxx)\n",
    "    plt.xlabel('Frequency (Hz)')\n",
    "    plt.ylabel('Power spectral density')\n",
    "    plt.xlim([0, 100])\n",
    "    plot_frequency_bands(ax)\n",
    "    plt.show()\n",
    "    \n",
    "def plot_group_boxplots (data, labels, xticks, colors=['blue', 'orange'], title='', xlabel='', ylabel='', yscale='linear', save=False):\n",
    "    def set_box_color(bp, color):\n",
    "        plt.setp(bp['boxes'], color=color)\n",
    "        plt.setp(bp['whiskers'], color=color)\n",
    "        plt.setp(bp['caps'], color=color)\n",
    "        plt.setp(bp['medians'], color=color)\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    \n",
    "    boxplots = []\n",
    "    for idx, data_group in enumerate(data):\n",
    "        shift = None\n",
    "        \n",
    "        if len(data) == 1:\n",
    "            shift = 0\n",
    "        elif len(data) == 2 and idx == 0:\n",
    "            shift = -0.4\n",
    "        elif len(data) ==2 and idx == 1:\n",
    "            shift = 0.4\n",
    "\n",
    "        positions = np.array(range(len(data_group))) * 2 + shift\n",
    "        flier = dict(marker='o', markerfacecolor='none', markeredgecolor=colors[idx], alpha=0.5)\n",
    "        bp = plt.boxplot(data_group, positions=positions, widths=0.6, flierprops=flier)\n",
    "        set_box_color(bp, colors[idx])\n",
    "        \n",
    "        boxplots.append(bp)\n",
    "    plt.xticks(range(0, len(xticks) * 2, 2), xticks)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.yscale(yscale)\n",
    "        \n",
    "    leg = plt.legend([bp[\"boxes\"][0] for bp in boxplots], labels, frameon=False, fontsize=15, bbox_to_anchor=(1, 1), loc='upper left')\n",
    "    for legobj in leg.legendHandles:\n",
    "        legobj.set_linewidth(2.0)\n",
    "        \n",
    "    ax = plt.gca()\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    \n",
    "    for item in ([ax.title] + ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "        item.set_fontsize(12.5)\n",
    "    for item in ([ax.xaxis.label, ax.yaxis.label] + ax.get_legend().get_texts()):\n",
    "        item.set_fontsize(15)\n",
    "    \n",
    "    if save:\n",
    "        plt.savefig(FIG_ROOT + save, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "#####################\n",
    "# Utility functions #\n",
    "#####################\n",
    "\n",
    "def get_spikes_in_range (data_range, spike_times):\n",
    "    spike_times = np.array(spike_times)\n",
    "    \n",
    "    start = data_range.start\n",
    "    stop = data_range.stop  \n",
    "    spike_time_idxs = np.where(np.logical_and(spike_times >= start, spike_times <= stop))[0]    \n",
    "    \n",
    "    return spike_times[spike_time_idxs]\n",
    "\n",
    "def get_slice_from_s (s_beg, s_end):\n",
    "    return slice(int(s_beg*SAMPLING_RATE), int(s_end*SAMPLING_RATE))\n",
    "\n",
    "def get_xticks (slice_val):\n",
    "    start, stop = slice_val.start, slice_val.stop\n",
    "    return np.linspace(start, stop, num=stop-start) / SAMPLING_RATE\n",
    "\n",
    "def save_data (data, filename=None):\n",
    "    path = ROOT\n",
    "    curr_time = datetime.today().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "    \n",
    "    if filename:\n",
    "        path = path + filename + '_' + curr_time + '.pickle'\n",
    "    else:\n",
    "        path = path + curr_time + '.pickle'\n",
    "    \n",
    "    with open(path, 'wb') as handle:\n",
    "        pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    print('Saved data as', path)\n",
    "    \n",
    "def open_data (filename):\n",
    "    path = os.path.join(ROOT, filename)\n",
    "    \n",
    "    with open(path, 'rb') as handle:\n",
    "        data = pickle.load(handle)\n",
    "        \n",
    "    return data\n",
    "    \n",
    "#########################################\n",
    "# Signal processing and stats functions #\n",
    "#########################################\n",
    "\n",
    "def welch_dof(x,y):\n",
    "    dof = (np.var(x)/len(x) + np.var(y)/len(y))**2 / ((np.var(x)/len(x))**2 / (len(x)-1) + (np.var(y)/len(y))**2 / (len(y)-1))\n",
    "    return dof \n",
    "\n",
    "def get_peaks (data):\n",
    "    min_prominence = 0.5*rms(data)\n",
    "    min_lag = int(SAMPLING_RATE * 0.025)\n",
    "    \n",
    "    peaks = signal.find_peaks(data, distance=min_lag, prominence=min_prominence)[0]\n",
    "    troughs = signal.find_peaks(-data, distance=min_lag, prominence=min_prominence)[0]\n",
    "\n",
    "    combined = np.array(sorted(np.concatenate([peaks, troughs])))\n",
    "\n",
    "    peak_ranks = []\n",
    "    trough_ranks = []\n",
    "\n",
    "    for point_idx, point in enumerate(combined):\n",
    "        if point in peaks:\n",
    "            peak_ranks.append(point_idx)\n",
    "        else:\n",
    "            trough_ranks.append(point_idx)  \n",
    "     \n",
    "    def get_max_height_from_ranks (ranks):\n",
    "        heights = [abs(data[combined[rank]]) for rank in ranks]\n",
    "        \n",
    "        return ranks[np.argmax(heights)]\n",
    "    \n",
    "    def get_nonadjacent_ranks (ranks):\n",
    "        nonadjacent_ranks = []\n",
    "        temp_ranks = []\n",
    "        for rank_idx, curr_rank in enumerate(ranks):\n",
    "            if rank_idx == 0:\n",
    "                temp_ranks.append(curr_rank)\n",
    "            else:\n",
    "                prev_rank = ranks[rank_idx-1]\n",
    "\n",
    "                if prev_rank+1 == curr_rank:\n",
    "                    temp_ranks.append(curr_rank)\n",
    "                else:\n",
    "                    nonadjacent_ranks.append(get_max_height_from_ranks(temp_ranks))\n",
    "                    temp_ranks = [curr_rank]\n",
    "\n",
    "            if rank_idx+1 == len(ranks):\n",
    "                nonadjacent_ranks.append(get_max_height_from_ranks(temp_ranks))\n",
    "\n",
    "        return nonadjacent_ranks\n",
    "        \n",
    "    filtered_ranks = sorted(\n",
    "        get_nonadjacent_ranks (peak_ranks) + \n",
    "        get_nonadjacent_ranks (trough_ranks)\n",
    "    )\n",
    "    filtered_points = combined[filtered_ranks]\n",
    "\n",
    "    return filtered_points, len(get_nonadjacent_ranks (peak_ranks))\n",
    "\n",
    "def twoway_anova (data, variable_labels):\n",
    "    factor_1_var, factor_2_var, value_var = variable_labels\n",
    "    \n",
    "    prepared_data = {}\n",
    "    prepared_data[factor_1_var] = []\n",
    "    prepared_data[factor_2_var] = []\n",
    "    prepared_data[value_var] = []\n",
    "    \n",
    "    for factor_1 in data.keys():\n",
    "        factor_2_values = data[factor_1]\n",
    "        for factor_2 in factor_2_values.keys():\n",
    "            values = factor_2_values[factor_2]\n",
    "            for value in values:\n",
    "                prepared_data[factor_1_var].append(factor_1) \n",
    "                prepared_data[factor_2_var].append(factor_2) \n",
    "                prepared_data[value_var].append(value) \n",
    "    \n",
    "    df = pd.DataFrame(prepared_data)\n",
    "    \n",
    "    formula = \"{value_var} ~ C({factor_1_var}) + C({factor_2_var}) + C({factor_1_var}):C({factor_2_var})\".format(\n",
    "        factor_1_var=factor_1_var,\n",
    "        factor_2_var=factor_2_var,\n",
    "        value_var=value_var\n",
    "    )\n",
    "    model = ols(formula, data=df).fit()\n",
    "    \n",
    "    return sm.stats.anova_lm(model, typ=2)\n",
    "\n",
    "def get_mean_psd (bursts):\n",
    "    fs = None\n",
    "    Pxxs = []\n",
    "    \n",
    "    for burst in bursts:\n",
    "        if hasattr(burst, \"normalized_psd\"):\n",
    "            f_burst, Pxx_burst = burst.normalized_psd\n",
    "            \n",
    "            fs = f_burst\n",
    "            Pxxs.append(Pxx_burst)\n",
    "\n",
    "    Pxx_mean = np.mean(Pxxs, axis=0)\n",
    "    Pxx_stderr = np.std(Pxxs, axis=0) / len(Pxxs)**0.5\n",
    "    \n",
    "    f, Pxx_mean = get_psd_in_range((fs, Pxx_mean), [0, 40])\n",
    "    f, Pxx_stderr = get_psd_in_range((fs, Pxx_stderr), [0, 40])\n",
    "    \n",
    "    return f, Pxx_mean, Pxx_stderr\n",
    "\n",
    "def butter_bandpass_filter(data, lowcut, highcut, order=3):\n",
    "    nyq = 0.5 * SAMPLING_RATE\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    sos = signal.butter(order, [low, high], analog=False, btype='bandpass', output='sos')\n",
    "    y = signal.sosfilt(sos, data)\n",
    "    return y\n",
    "\n",
    "def butter_lowpass_filter(data, lowcut, order=3):\n",
    "    nyq = 0.5 * SAMPLING_RATE\n",
    "    low = lowcut / nyq\n",
    "    sos = signal.butter(order, low, analog=False, btype='lowpass', output='sos')\n",
    "    y = signal.sosfilt(sos, data)\n",
    "    return y\n",
    "\n",
    "def butter_highpass_filter(data, highcut, order=3):\n",
    "    nyq = 0.5 * SAMPLING_RATE\n",
    "    high = highcut / nyq\n",
    "    sos = signal.butter(order, high, analog=False, btype='highpass', output='sos')\n",
    "    y = signal.sosfilt(sos, data)\n",
    "    return y\n",
    "\n",
    "def scale_0_to_1 (data):\n",
    "    return (data - np.min(data)) / (np.max(data) - np.min(data))\n",
    "\n",
    "def multitaper_psd (data, NW=3, k=5, resample_freq=1000, show_progress=False):\n",
    "    # Resample to 1000 Hz to speed up multitaper\n",
    "    data = signal.resample(data, int(len(data)*resample_freq/SAMPLING_RATE))\n",
    "    \n",
    "    # How many seconds for each window (N = fs*duration)\n",
    "    window = 1\n",
    "    N = int(resample_freq*window)\n",
    "    [tapers, eigen] = dpss(N, NW, k)\n",
    "    \n",
    "    Pxx_list = []\n",
    "    \n",
    "    last_progress = 0\n",
    "    \n",
    "    # Proceed through signal advancing in steps of size N\n",
    "    for idx in range(0, len(data), N//10):\n",
    "        if show_progress:\n",
    "            progress = (idx/resample_freq)//60\n",
    "            if last_progress != progress:\n",
    "                last_progress = progress\n",
    "                print('Processed {} mins'.format(progress))\n",
    "        \n",
    "        y = data[idx:idx+N]\n",
    "        \n",
    "        # For a constant window, pad data with zeros to make sure\n",
    "        # each window is of length N\n",
    "        if len(y) < N:\n",
    "            padding = N - len(y)\n",
    "            y = np.concatenate( (y, np.zeros(padding)) )\n",
    "        \n",
    "        Sk_complex, weights, eigenvalues = pmtm(y, e=eigen, v=tapers, show=False)\n",
    "        Sk = abs(Sk_complex)**2\n",
    "        Sk = np.mean(Sk * np.transpose(weights), axis=0)\n",
    "\n",
    "        Pxx_list.append( Sk[0:N//2] )\n",
    "        \n",
    "    # Get list of frequencies to accompany PSD\n",
    "    dt = 1.0/resample_freq\n",
    "    f = np.linspace(0.0, 1.0/(2.0*dt), N//2)\n",
    "\n",
    "    # Average the PSD over all windows\n",
    "    Pxx = np.mean(Pxx_list, axis=0)\n",
    "    \n",
    "    return f, Pxx\n",
    "\n",
    "def get_psd_in_range (PSD, freqs):\n",
    "    f, Pxx = PSD\n",
    "    \n",
    "    f_low = np.where(f >= freqs[0])[0][0]\n",
    "    f_high = np.where(f >= freqs[1])[0][0]\n",
    "    \n",
    "    return f[f_low:f_high], Pxx[f_low:f_high]\n",
    "\n",
    "def get_mean_PSD_freq (PSD):\n",
    "    max_freq = 80    \n",
    "    f, Pxx = PSD\n",
    "    \n",
    "    max_freq_idx = np.where(f <= 80)[0][-1]\n",
    "    return np.sum(f[:max_freq_idx]*Pxx[:max_freq_idx])/np.sum(Pxx[:max_freq_idx])\n",
    "\n",
    "def get_relative_power (PSD):\n",
    "    f, Pxx = PSD\n",
    "    \n",
    "    max_idx = np.where(f >= 100)[0][0]\n",
    "    f = f[:max_idx]\n",
    "    Pxx = Pxx[:max_idx]\n",
    "    \n",
    "    spindle_idxs = np.where(np.logical_and(f > 8, f <= 30))\n",
    "    spindle_power = np.sum(Pxx[spindle_idxs]) / sum(Pxx)\n",
    "\n",
    "    gamma_idxs = np.where(np.logical_and(f > 30, f <= 80))\n",
    "    gamma_power = np.sum(Pxx[gamma_idxs]) / sum(Pxx)\n",
    "\n",
    "    return spindle_power, gamma_power\n",
    "\n",
    "def get_maximal_frequency (PSD):\n",
    "    f, Pxx = PSD\n",
    "    \n",
    "    # Discard DC value\n",
    "    f = f[1:]\n",
    "    Pxx = Pxx[1:]\n",
    "\n",
    "    return f[np.argmax(Pxx)]\n",
    "\n",
    "######################\n",
    "# Constant variables #\n",
    "######################\n",
    "\n",
    "SAMPLING_RATE    = 30000\n",
    "ROOT             = '/' # Where to load data\n",
    "FIG_ROOT         = '/' # Where to save figures\n",
    "DATA_RANGE_ALL   = get_slice_from_s(0, 60*60) \n",
    "\n",
    "COLOR_CORTEX   = 'tab:green'\n",
    "COLOR_THALAMUS = 'tab:purple'\n",
    "COLOR_STRIATUM = 'tab:orange'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data structure for recordings used in analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RECORDINGS = [\n",
    "    {\n",
    "        \"path\": \"2020-09-01_15-56-56\",\n",
    "        \"age\":  6,\n",
    "        \"thalamus\": False,\n",
    "        \"striatum_channel\": 2,\n",
    "        \"cortex_channel\": 14,\n",
    "        \"recording\": 1,\n",
    "        \"striatum_sigma\": 2.125,\n",
    "        \"cortex_sigma\": 2.125\n",
    "    },\n",
    "         {\n",
    "        \"path\": \"2020-09-30_13-40-33\",\n",
    "        \"age\": 10,\n",
    "        \"thalamus\": False,\n",
    "        \"striatum_channel\": 2,\n",
    "        \"cortex_channel\": 14,\n",
    "        \"recording\": 0,\n",
    "        \"striatum_sigma\": 1.25,\n",
    "        \"cortex_sigma\": 1.25\n",
    "    },\n",
    "    { \n",
    "        \"path\": \"2020-10-05_13-54-29\",\n",
    "        \"age\": 15,\n",
    "        \"thalamus\": False,\n",
    "        \"striatum_channel\": 2,\n",
    "        \"cortex_channel\": 15,\n",
    "        \"recording\": 0,\n",
    "        \"striatum_sigma\": 1.75,\n",
    "        \"cortex_sigma\": 1.625\n",
    "    },\n",
    "    {\n",
    "        \"path\": \"2020-10-28_15-19-52\",\n",
    "        \"age\": 10,\n",
    "        \"thalamus\": False,\n",
    "        \"striatum_channel\": 2,\n",
    "        \"cortex_channel\": 0,\n",
    "        \"recording\": 0,\n",
    "        \"striatum_sigma\": 1.25,\n",
    "        \"cortex_sigma\": 1.25\n",
    "    },\n",
    "    {\n",
    "        \"path\": \"2021-02-09_16-08-16\",\n",
    "        \"age\":  8,\n",
    "        \"thalamus\": False,\n",
    "        \"striatum_channel\": 2,\n",
    "        \"cortex_channel\": 15,\n",
    "        \"recording\": 0,\n",
    "        \"striatum_sigma\": 1.75,\n",
    "        \"cortex_sigma\": 1.75\n",
    "    },\n",
    "    {\n",
    "        \"path\": \"2021-02-10_15-32-43\",\n",
    "        \"age\":  9,\n",
    "        \"thalamus\": False,\n",
    "        \"striatum_channel\": 13,\n",
    "        \"cortex_channel\": 15,\n",
    "        \"recording\": 0,\n",
    "        \"striatum_sigma\": 2,\n",
    "        \"cortex_sigma\": 1.5\n",
    "    },\n",
    "    \n",
    "    { \n",
    "        \"path\": \"2021-02-11_15-57-01\", \n",
    "        \"age\": 10, \n",
    "        \"thalamus\": True, \n",
    "        \"striatum_channel\": 2,\n",
    "        \"thalamus_channel\": 18,\n",
    "        \"cortex_channel\": 15,\n",
    "        \"recording\": 0,\n",
    "        \"striatum_sigma\": 1.75, \n",
    "        \"thalamus_sigma\": 1.75,\n",
    "        \"cortex_sigma\": 1.25\n",
    "    },\n",
    "    { \n",
    "        \"path\": \"2021-02-12_15-25-59\", \n",
    "        \"age\": 11, \n",
    "        \"thalamus\": True, \n",
    "        \"striatum_channel\": 2,\n",
    "        \"thalamus_channel\": 18,\n",
    "        \"cortex_channel\": 15,\n",
    "        \"recording\": 0,\n",
    "        \"striatum_sigma\": 1.25, \n",
    "        \"thalamus_sigma\": 1.25,\n",
    "        \"cortex_sigma\": 1.25\n",
    "    },\n",
    "    { \n",
    "        \"path\": \"2021-03-01_11-16-08\",\n",
    "        \"age\":  5, \n",
    "        \"thalamus\": True,\n",
    "        \"striatum_channel\": 13,\n",
    "        \"thalamus_channel\": 18,\n",
    "        \"cortex_channel\": 0,\n",
    "        \"recording\": 0,\n",
    "        \"striatum_sigma\": 2.875, \n",
    "        \"thalamus_sigma\": 3.5,\n",
    "        \"cortex_sigma\": 2.75\n",
    "    },\n",
    "    { \n",
    "        \"path\": \"2021-03-03_11-02-08\",\n",
    "        \"age\":  7,\n",
    "        \"thalamus\": True,\n",
    "        \"striatum_channel\": 2,\n",
    "        \"thalamus_channel\": 29,\n",
    "        \"cortex_channel\": 0,\n",
    "        \"recording\": 0,\n",
    "        \"striatum_sigma\": 1, \n",
    "        \"thalamus_sigma\": 0.55,\n",
    "        \"cortex_sigma\": 0.75\n",
    "    },\n",
    "    { \n",
    "        \"path\": \"2021-03-05_10-58-31\", \n",
    "        \"age\":  9, \n",
    "        \"thalamus\": True, \n",
    "        \"striatum_channel\": 2,\n",
    "        \"thalamus_channel\": 18,\n",
    "        \"cortex_channel\": 0,\n",
    "        \"recording\": 0,\n",
    "        \"striatum_sigma\": 1.75, \n",
    "        \"thalamus_sigma\": 2.75,\n",
    "        \"cortex_sigma\": 1.75\n",
    "    },\n",
    "    { \n",
    "        \"path\": \"2021-03-08_10-50-18\", \n",
    "        \"age\": 12,\n",
    "        \"thalamus\": True, \n",
    "        \"striatum_channel\": 2,\n",
    "        \"thalamus_channel\": 18,\n",
    "        \"cortex_channel\": 15,\n",
    "        \"recording\": 0,\n",
    "        \"striatum_sigma\": 1.25, \n",
    "        \"thalamus_sigma\": 1.75,\n",
    "        \"cortex_sigma\": 1.25\n",
    "    },\n",
    "    { \n",
    "        \"path\": \"2021-03-10_10-59-57\",\n",
    "        \"age\": 14,\n",
    "        \"thalamus\": True,\n",
    "        \"striatum_channel\": 2,\n",
    "        \"thalamus_channel\": 18,\n",
    "        \"cortex_channel\": 15,\n",
    "        \"recording\": 0,\n",
    "        \"striatum_sigma\": 1.25, \n",
    "        \"thalamus_sigma\": 1.75,\n",
    "        \"cortex_sigma\": 1.25\n",
    "    },\n",
    "    {\n",
    "        \"path\": \"2021-03-11_10-48-40\",\n",
    "        \"age\":  8, \n",
    "        \"thalamus\": True, \n",
    "        \"striatum_channel\": 2,\n",
    "        \"thalamus_channel\": 18,\n",
    "        \"cortex_channel\": 0,\n",
    "        \"recording\": 1,\n",
    "        \"striatum_sigma\": 2.5, \n",
    "        \"thalamus_sigma\": 3,\n",
    "        \"cortex_sigma\": 2.5\n",
    "    },\n",
    "    { \n",
    "        \"path\": \"2021-03-12_10-54-00\", \n",
    "        \"age\":  9, \n",
    "        \"thalamus\": True, \n",
    "        \"striatum_channel\": 13, \n",
    "        \"thalamus_channel\": 29, \n",
    "        \"cortex_channel\": 15,\n",
    "        \"recording\": 1,\n",
    "        \"striatum_sigma\": 2.125,\n",
    "        \"thalamus_sigma\": 2.75,\n",
    "        \"cortex_sigma\": 2.125\n",
    "    },\n",
    "    {\n",
    "        \"path\": \"2021-03-15_10-52-05\", \n",
    "        \"age\": 12,\n",
    "        \"thalamus\": True, \n",
    "        \"striatum_channel\": 2,\n",
    "        \"thalamus_channel\": 18,\n",
    "        \"cortex_channel\": 0,\n",
    "        \"recording\": 0,\n",
    "        \"striatum_sigma\": 1.25, \n",
    "        \"thalamus_sigma\": 1.75,\n",
    "        \"cortex_sigma\": 1.25\n",
    "    },\n",
    "    { \n",
    "        \"path\": \"2021-05-14_15-40-25\",\n",
    "        \"age\": 10, \n",
    "        \"thalamus\": True, \n",
    "        \"striatum_channel\": 2,\n",
    "        \"thalamus_channel\": 18,\n",
    "        \"cortex_channel\": 0,\n",
    "        \"recording\": 0,\n",
    "        \"striatum_sigma\": 1.25, \n",
    "        \"thalamus_sigma\": 1.75,\n",
    "        \"cortex_sigma\": 1.25\n",
    "    },\n",
    "    { \n",
    "        \"path\": \"2021-05-21_15-10-39\",\n",
    "        \"age\": 12, \n",
    "        \"thalamus\": True, \n",
    "        \"striatum_channel\": 2,\n",
    "        \"thalamus_channel\": 18,\n",
    "        \"cortex_channel\": 15,\n",
    "        \"recording\": 0,\n",
    "        \"striatum_sigma\": 1.75, \n",
    "        \"thalamus_sigma\": 1.75,\n",
    "        \"cortex_sigma\": 1.25\n",
    "    },\n",
    "    { \n",
    "        \"path\": \"2021-05-26_15-12-59\", \n",
    "        \"age\": 10, \n",
    "        \"thalamus\": True,\n",
    "        \"striatum_channel\": 2,\n",
    "        \"thalamus_channel\": 29,\n",
    "        \"cortex_channel\": 15,\n",
    "        \"recording\": 0,\n",
    "        \"striatum_sigma\": 2.625,\n",
    "        \"thalamus_sigma\": 2.375,\n",
    "        \"cortex_sigma\": 2.125\n",
    "    },\n",
    "    { \n",
    "        \"path\": \"2021-08-03_15-52-34\", \n",
    "        \"age\": 5, \n",
    "        \"thalamus\": True,\n",
    "        \"striatum_channel\": 2,\n",
    "        \"thalamus_channel\": 29,\n",
    "        \"cortex_channel\": 15,\n",
    "        \"recording\": 1\n",
    "    }, \n",
    "    {\n",
    "        \"path\": \"2021-08-09_10-14-01\",\n",
    "        \"age\": 6,\n",
    "        \"thalamus\": True,\n",
    "        \"striatum_channel\": 2,\n",
    "        \"thalamus_channel\": 18,\n",
    "        \"cortex_channel\": 15,\n",
    "        \"recording\": 0\n",
    "    },\n",
    "    {\n",
    "        \"path\": \"2020-10-26_14-03-44\",\n",
    "        \"age\": 15,\n",
    "        \"recording\": 0,\n",
    "        \"striatum_channel\": 2,\n",
    "        \"thalamus_channel\": 29,\n",
    "        \"cortex_channel\": 15\n",
    "    },\n",
    "    {\n",
    "        \"path\": \"2021-06-25_11-20-59\",\n",
    "        \"age\": 16,\n",
    "        \"recording\": 0,\n",
    "        \"striatum_channel\": 2,\n",
    "        \"thalamus_channel\": 29,\n",
    "        \"cortex_channel\": 15\n",
    "    }\n",
    "]\n",
    "\n",
    "# Sort by age\n",
    "RECORDINGS = sorted(RECORDINGS, key=lambda k: k['age']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Burst detection\n",
    "### Envelope thresholding method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_bursts_from_envelope(envelope, low_threshold):\n",
    "    bursts = []\n",
    "    \n",
    "    start_t = False\n",
    "    burst = []\n",
    "    \n",
    "    for t, sample in enumerate(envelope):\n",
    "        if sample > low_threshold:\n",
    "            if not start_t:\n",
    "                start_t = t\n",
    "            burst.append(sample)\n",
    "            \n",
    "        if start_t and (sample <= low_threshold or t+1 == len(envelope)):\n",
    "            bursts.append( Burst([start_t, t], burst) )\n",
    "            start_t = False\n",
    "            burst = []\n",
    "    \n",
    "    return bursts\n",
    "        \n",
    "\n",
    "# Filter bursts shorter than minimum duration\n",
    "def filter_short_bursts (bursts, minimum_duration):\n",
    "    filtered_bursts = []\n",
    "    \n",
    "    for burst in bursts:\n",
    "        if (burst.time[1] - burst.time[0]) > (minimum_duration*SAMPLING_RATE):\n",
    "            filtered_bursts.append(burst)\n",
    "            \n",
    "    return filtered_bursts\n",
    "\n",
    "# Filter bursts that have fewer than minimum points above threshold\n",
    "def filter_minimum_peaks (bursts, threshold, minimum_peaks):\n",
    "    filtered_bursts = []\n",
    "    \n",
    "    for burst in bursts:\n",
    "        peaks = signal.find_peaks(np.abs(burst.data), prominence=threshold)[0]\n",
    "        diff_count = 0\n",
    "        \n",
    "        for peak_idx, peak in enumerate(peaks):\n",
    "            if peak_idx+1 == len(peaks):\n",
    "                continue\n",
    "                \n",
    "            next_peak = peaks[peak_idx+1]\n",
    "            diff = (next_peak - peak)/SAMPLING_RATE\n",
    "\n",
    "            if diff_count == (minimum_peaks-1):\n",
    "                break\n",
    "            \n",
    "            if diff < 0.2:\n",
    "                diff_count += 1\n",
    "            else:\n",
    "                diff = 0\n",
    "                \n",
    "        \n",
    "        if diff_count == minimum_peaks-1:\n",
    "            filtered_bursts.append(burst)\n",
    "            \n",
    "    return filtered_bursts\n",
    "\n",
    "# If bursts have been bandpass filtered, use this function\n",
    "# to return unfiltered bursts using saved time points for each burst\n",
    "def get_unfiltered_bursts (bursts, data):\n",
    "    unfiltered_bursts = []\n",
    "    \n",
    "    for burst in bursts:\n",
    "        burst_slice = slice(*burst.time)\n",
    "        unfiltered_bursts.append( Burst(burst.time, data[burst_slice]) )\n",
    "\n",
    "    return unfiltered_bursts\n",
    "\n",
    "def get_baseline_periods (bursts, data):\n",
    "    baseline_bursts = []\n",
    "    \n",
    "    padding = int(SAMPLING_RATE * 0.5)\n",
    "    \n",
    "    for idx, burst in enumerate(bursts):\n",
    "        # If on final burst of set\n",
    "        if (idx+1) == len(bursts):\n",
    "            continue\n",
    "            \n",
    "        next_burst_time = bursts[idx+1][0]\n",
    "        curr_burst_time = burst[0]\n",
    "        time_diff = next_burst_time[0] - curr_burst_time[1]\n",
    "\n",
    "        if time_diff > (padding*4):\n",
    "            start_t = next_burst_time[0] + padding\n",
    "            end_t = curr_burst_time[1] - padding\n",
    "            baseline_data = data[start_t:end_t]\n",
    "            baseline_bursts.append( ([start_t, end_t], baseline_data) )\n",
    "\n",
    "    return baseline_bursts\n",
    "\n",
    "def combined_adjacent_bursts(bursts, data, temporal_distance):\n",
    "    combined_bursts = []\n",
    "    temp_bursts = []\n",
    "    \n",
    "    # Take first and last bursts and combine\n",
    "    def combine_bursts (temp_bursts):\n",
    "        start = temp_bursts[0].time[0]\n",
    "        end = temp_bursts[-1].time[1]\n",
    "        \n",
    "        return Burst([start, end], data[start:end])\n",
    "    \n",
    "    for burst_idx, burst in enumerate(bursts):    \n",
    "        if burst_idx == 0:\n",
    "            continue\n",
    "            \n",
    "        prev = bursts[burst_idx-1]\n",
    "        \n",
    "        prev_time = prev.time\n",
    "        curr_time = burst.time\n",
    "        \n",
    "        temp_bursts.append(prev)\n",
    "        \n",
    "        # If bursts are greater than 1 second apart\n",
    "        if (curr_time[0] - prev_time[1] > temporal_distance*SAMPLING_RATE):\n",
    "            combined_bursts.append(combine_bursts(temp_bursts))\n",
    "            temp_bursts = []\n",
    "        \n",
    "        # Or if on final burst of set\n",
    "        if burst_idx+1 == len(bursts):\n",
    "            temp_bursts.append(burst)\n",
    "            \n",
    "            combined_bursts.append(combine_bursts(temp_bursts))\n",
    "            temp_bursts = []\n",
    "            \n",
    "    return combined_bursts\n",
    "\n",
    "\n",
    "def hl_envelopes_idx(s, dmin=1, dmax=1):\n",
    "    # locals min      \n",
    "    lmin = (np.diff(np.sign(np.diff(s))) > 0).nonzero()[0] + 1 \n",
    "    # locals max\n",
    "    lmax = (np.diff(np.sign(np.diff(s))) < 0).nonzero()[0] + 1 \n",
    "\n",
    "    # global max of dmax-chunks of locals max \n",
    "    lmin = lmin[[i+np.argmin(s[lmin[i:i+dmin]]) for i in range(0,len(lmin),dmin)]]\n",
    "    # global min of dmin-chunks of locals min \n",
    "    lmax = lmax[[i+np.argmax(s[lmax[i:i+dmax]]) for i in range(0,len(lmax),dmax)]]\n",
    "    \n",
    "    return lmin,lmax\n",
    "            \n",
    "\n",
    "# Cobmines all previous functions into a single routine\n",
    "# Returns list of burst tuples in form ([start time, end time], [burst data])\n",
    "\n",
    "# Samples              Array of sampled data\n",
    "# Sigma                How many standard deviations above mean for threshold\n",
    "# Minimum_duration     Minimum burst duration to keep\n",
    "\n",
    "def run_burst_procedure (data, minimum_peaks, minimum_duration, sigma):\n",
    "    # Bandpass filter signal\n",
    "    data_bandpass = butter_bandpass_filter(data, lowcut=1, highcut=100)\n",
    "        \n",
    "    # Get envelope of signal\n",
    "    low_idx, high_idx = hl_envelopes_idx(data_bandpass, dmin=30, dmax=30)\n",
    "    x = np.arange(0, len(data_bandpass))\n",
    "    high_env = np.interp(x, x[high_idx], data_bandpass[high_idx])\n",
    "    low_env = np.interp(x, x[low_idx], data_bandpass[low_idx])\n",
    "    mean_env = []\n",
    "    for t, _ in enumerate(high_env):\n",
    "        a, b = high_env[t], abs(low_env[t])\n",
    "        if a > b:\n",
    "            mean_env.append(a)\n",
    "        else:\n",
    "            mean_env.append(b)\n",
    "    \n",
    "    # Clip data to prevent skew of std from random outlier events\n",
    "    data_clipped = np.clip(data_bandpass, a_min=-1000, a_max=1000)\n",
    "    \n",
    "    # Get thresholds\n",
    "    mean, std = np.mean(data_clipped), np.std(data_clipped)\n",
    "    burst_threshold = mean + std*sigma\n",
    "    print('Calculated threshold ({}, sigma={})\\n'.format(\n",
    "        round(burst_threshold, 2),\n",
    "        round(sigma, 2)\n",
    "    ))\n",
    "\n",
    "    bursts = detect_bursts_from_envelope(mean_env, burst_threshold)\n",
    "    print('\\nBursts detected')\n",
    "    \n",
    "    bursts = combined_adjacent_bursts(bursts, mean_env, temporal_distance=0.2)\n",
    "    print('Combined adjacent bursts')\n",
    "    \n",
    "    bursts = filter_rms(bursts, max_rms=1000)\n",
    "    print('\\tFiltered RMS')\n",
    "\n",
    "    bursts = filter_short_bursts(bursts, minimum_duration)\n",
    "    print('Short bursts filtered')\n",
    "    \n",
    "    bursts = get_unfiltered_bursts(bursts, data_bandpass)\n",
    "    print('Got unfiltered bursts')\n",
    "    \n",
    "    bursts = filter_minimum_peaks(bursts, burst_threshold, minimum_peaks=minimum_peaks)\n",
    "    print('Filtered minimum thresold points')\n",
    "    \n",
    "    return bursts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data (recordings, get_mua=False, get_psd=False):\n",
    "    bursts = []\n",
    "    \n",
    "    for idx, recording in enumerate(recordings[:17]):\n",
    "        print(\"\\nRecording {} ({}/{})\".format(\n",
    "            recording[\"path\"],\n",
    "            idx+1,\n",
    "            len(recordings[:17])\n",
    "        ))\n",
    "        \n",
    "        data_range = get_slice_from_s(0, 60*60)\n",
    "        recording_n = recording[\"recording\"]\n",
    "        striatum_channel_n = recording[\"striatum_channel\"]\n",
    "        thalamus_channel_n = recording[\"thalamus_channel\"] if recording[\"thalamus\"] else 0\n",
    "        cortex_channel_n = recording[\"cortex_channel\"]\n",
    "\n",
    "        session = Session(ROOT + recording[\"path\"])\n",
    "        striatum_data = session.recordings[recording_n].continuous[0].samples[data_range, striatum_channel_n]\n",
    "        thalamus_data = session.recordings[recording_n].continuous[0].samples[data_range, thalamus_channel_n]\n",
    "        cortex_data = session.recordings[recording_n].continuous[0].samples[data_range, cortex_channel_n]\n",
    "\n",
    "        minimum_duration = 0.2\n",
    "        minimum_peaks = 10\n",
    "        \n",
    "        recording = recording.copy()\n",
    "        \n",
    "        # Striatum\n",
    "        if len(striatum_data):\n",
    "            recording[\"length\"] = len(striatum_data)\n",
    "            recording[\"striatum_bursts\"]  = run_burst_procedure(\n",
    "                data=striatum_data,\n",
    "                minimum_peaks=minimum_peaks,\n",
    "                minimum_duration=minimum_duration,\n",
    "                sigma=recording[\"striatum_sigma\"]\n",
    "            )\n",
    "            if get_mua:\n",
    "                recording[\"striatum_MUA\"] = detect_MUA(\n",
    "                    data=striatum_data\n",
    "                )\n",
    "            if get_psd:\n",
    "                recording[\"striatum_PSD\"] = multitaper_psd (\n",
    "                    striatum_data,\n",
    "                    NW=3, k=5, resample_freq=1000,\n",
    "                    show_progress=True\n",
    "                )\n",
    "                \n",
    "        # Thalamus\n",
    "        if len(thalamus_data) and recording[\"thalamus\"]:\n",
    "            recording[\"thalamus_bursts\"] = run_burst_procedure(\n",
    "                data=thalamus_data,\n",
    "                minimum_peaks=minimum_peaks,\n",
    "                minimum_duration=minimum_duration,\n",
    "                sigma=recording[\"thalamus_sigma\"]\n",
    "            )\n",
    "            if get_mua:\n",
    "                recording[\"thalamus_MUA\"] = detect_MUA(\n",
    "                    data=thalamus_data\n",
    "                )\n",
    "            if get_psd:\n",
    "                recording[\"thalamus_PSD\"] = multitaper_psd (\n",
    "                    thalamus_data,\n",
    "                    NW=3, k=5, resample_freq=1000,\n",
    "                    show_progress=True\n",
    "                )\n",
    "        \n",
    "        # Cortex\n",
    "        if len(cortex_data):\n",
    "            recording[\"cortex_bursts\"]  = run_burst_procedure(\n",
    "                data=cortex_data,\n",
    "                minimum_peaks=minimum_peaks,\n",
    "                minimum_duration=minimum_duration,\n",
    "                sigma=recording[\"cortex_sigma\"]\n",
    "            )\n",
    "            if get_mua:\n",
    "                recording[\"cortex_MUA\"] = detect_MUA(\n",
    "                    data=cortex_data\n",
    "                )\n",
    "            if get_psd:\n",
    "                recording[\"cortex_PSD\"] = multitaper_psd (\n",
    "                    cortex_data,\n",
    "                    NW=3, k=5, resample_freq=1000,\n",
    "                    show_progress=True\n",
    "                )\n",
    "\n",
    "        bursts.append(recording)\n",
    "    \n",
    "    return bursts\n",
    "        \n",
    "processed_recordings = load_data(RECORDINGS, get_mua=False, get_psd=False)\n",
    "save_data(processed_recordings, 'processed_recordings_envelope')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Burst detection\n",
    "### RMS thresholding method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rms (data):\n",
    "    square = [d**2 for d in data]\n",
    "    mean = np.mean(square)\n",
    "    root = mean**0.5\n",
    "    return root\n",
    "\n",
    "def rms_hist(data, window=0.2):\n",
    "    rms_list = []\n",
    "    \n",
    "    chunk_size = int(SAMPLING_RATE*window) # Convert window in s to samples\n",
    "    for i in range(0, len(data), chunk_size):\n",
    "        data_chunk = data[i:i+chunk_size]\n",
    "        rms_list.append(rms(data_chunk))\n",
    "    \n",
    "    n_bins = int(np.percentile(rms_list, 85))\n",
    "    hist, bin_edges = np.histogram(rms_list, density=True, bins=n_bins)\n",
    "    \n",
    "    return hist, bin_edges, rms_list\n",
    "\n",
    "def plot_gaussian (bin_centres, hist, hist_fit):\n",
    "    fig = plt.figure()\n",
    "    plt.plot(bin_centres, hist, label='RMS', c='tab:red')\n",
    "    plt.plot(bin_centres, hist_fit, label='Fitted Gaussian', c='black')\n",
    "    plt.xlabel('RMS')\n",
    "    plt.ylabel('Density')\n",
    "    \n",
    "    leg = plt.legend(frameon=False, fontsize=15, bbox_to_anchor=(0.925, 1), loc='upper left')\n",
    "    for legobj in leg.legendHandles:\n",
    "        legobj.set_linewidth(2.0)\n",
    "        \n",
    "    ax = plt.gca()\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    \n",
    "    for item in ([ax.title] + ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "        item.set_fontsize(12.5)\n",
    "    for item in ([ax.xaxis.label, ax.yaxis.label] + ax.get_legend().get_texts()):\n",
    "        item.set_fontsize(15)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def fit_gaussian(hist, bin_edges, use_truncated_hist):\n",
    "    bin_centres = (bin_edges[:-1] + bin_edges[1:])/2\n",
    "    \n",
    "    if use_truncated_hist:\n",
    "        hist_30_idx = np.where(bin_centres <= 30)[0][-1]\n",
    "        peak_idx = np.argmax(hist[:hist_30_idx])\n",
    "    else:\n",
    "        peak_idx = np.argmax(hist)\n",
    "    peak = bin_centres[peak_idx]\n",
    "\n",
    "    trimmed_idx = max(4, peak_idx*2)\n",
    "    trimmed_bin_centres = bin_centres[:trimmed_idx]\n",
    "    trimmed_hist = hist[:trimmed_idx]\n",
    "\n",
    "    def gauss(x, *p):\n",
    "        A, mu, sigma = p\n",
    "        return A*np.exp(-(x-mu)**2/(2.*sigma**2))\n",
    "\n",
    "    p0 = [1., peak, 1.]\n",
    "    \n",
    "    coeff, var_matrix = curve_fit(gauss, trimmed_bin_centres, trimmed_hist, p0=p0)\n",
    "    \n",
    "    hist_fit = gauss(bin_centres, *coeff)\n",
    "    \n",
    "    plot_gaussian (bin_centres, hist, hist_fit)\n",
    "    \n",
    "    return coeff[1], coeff[2]\n",
    "\n",
    "def get_burst_events (data, rms_list, mu, sigma, window=0.2):\n",
    "    bursts = []\n",
    "    threshold = mu + 2*sigma\n",
    "    \n",
    "    chunk_size = int(SAMPLING_RATE*window) # Convert window in s to samples\n",
    "    for chunk_idx, chunk_rms in enumerate(rms_list):\n",
    "        if chunk_rms > threshold:\n",
    "            t_start = chunk_idx*chunk_size\n",
    "            t_end = min(len(data), (chunk_idx+1)*chunk_size)\n",
    "            \n",
    "            burst = Burst([t_start, t_end], data[t_start:t_end])\n",
    "            bursts.append(burst)\n",
    "    \n",
    "    return bursts\n",
    "\n",
    "def combine_adjacent_bursts(data, bursts, temporal_distance):\n",
    "    combined_bursts = []\n",
    "    temp_bursts = []\n",
    "    \n",
    "    # Take first and last bursts and combine\n",
    "    def combine_bursts (temp_bursts):\n",
    "        start = temp_bursts[0].time[0]\n",
    "        end = temp_bursts[-1].time[1]\n",
    "        \n",
    "        return Burst([start, end], data[start:end])\n",
    "    \n",
    "    for burst_idx, burst in enumerate(bursts):    \n",
    "        if burst_idx == 0:\n",
    "            continue\n",
    "            \n",
    "        prev = bursts[burst_idx-1]\n",
    "        \n",
    "        prev_time = prev.time\n",
    "        curr_time = burst.time\n",
    "        \n",
    "        temp_bursts.append(prev)\n",
    "        \n",
    "        # If bursts are greater than x seconds apart\n",
    "        if (curr_time[0] - prev_time[1] > temporal_distance*SAMPLING_RATE):\n",
    "            combined_bursts.append(combine_bursts(temp_bursts))\n",
    "            temp_bursts = []\n",
    "        \n",
    "        # Or if on final burst of set\n",
    "        if burst_idx+1 == len(bursts):\n",
    "            temp_bursts.append(burst)\n",
    "            \n",
    "            combined_bursts.append(combine_bursts(temp_bursts))\n",
    "            temp_bursts = []\n",
    "            \n",
    "    return combined_bursts\n",
    "\n",
    "def filter_short_bursts (bursts, minimum_duration):\n",
    "    minimum_samples = minimum_duration*SAMPLING_RATE\n",
    "    \n",
    "    return [b for b in bursts if (b.time[1]-b.time[0]) >= minimum_samples]\n",
    "\n",
    "def filter_rms (bursts, max_rms):\n",
    "    filtered_bursts = []\n",
    "    \n",
    "    for burst in bursts:\n",
    "        rms_list = []\n",
    "        chunk_size = int(SAMPLING_RATE*0.2) # Convert window in s to samples\n",
    "        for i in range(0, len(burst.data), chunk_size):\n",
    "            data_chunk = burst.data[i:i+chunk_size]\n",
    "            rms_list.append(rms(data_chunk))\n",
    "        if np.max(rms_list) < max_rms:\n",
    "            filtered_bursts.append(burst)\n",
    "    \n",
    "    return filtered_bursts\n",
    "\n",
    "def filter_min_peaks (bursts, min_peaks):\n",
    "    filtered_bursts = []\n",
    "    \n",
    "    for burst in bursts:\n",
    "        _, n_peaks = get_peaks(burst.data)\n",
    "        \n",
    "        if n_peaks >= min_peaks:\n",
    "            filtered_bursts.append(burst)\n",
    "    \n",
    "    return filtered_bursts\n",
    "\n",
    "def get_raw_bursts (data, bursts):\n",
    "    unfiltered_bursts = []\n",
    "\n",
    "    for burst in bursts:\n",
    "        unfiltered_burst = Burst(\n",
    "            burst.time,\n",
    "            data[burst.time[0]:burst.time[1]]\n",
    "        )\n",
    "        unfiltered_bursts.append(unfiltered_burst)\n",
    "    \n",
    "    return unfiltered_bursts\n",
    "\n",
    "def get_baseline_power (data, bursts):\n",
    "    # Get baseline periods\n",
    "    baseline_data = []\n",
    "    for burst_idx, burst in enumerate(bursts):\n",
    "        # If on last burst, baseline will last until end of recording\n",
    "        if burst_idx+1 == len(bursts):\n",
    "            next_burst_time = [len(data)]\n",
    "        else:\n",
    "            next_burst_time = bursts[burst_idx+1].time\n",
    "\n",
    "        curr_burst_time = burst.time  \n",
    "        \n",
    "        baseline_period = data[curr_burst_time[1]:next_burst_time[0]]\n",
    "        # Minimum length of 1s\n",
    "        if len(baseline_period)/SAMPLING_RATE >= 1:\n",
    "            # Trim to max 10s\n",
    "            baseline_period = baseline_period[get_slice_from_s(0, 5)]\n",
    "            baseline_data.append(baseline_period)\n",
    "            \n",
    "    # Take mean of all baseline psd's\n",
    "    baseline_psds = []\n",
    "    for baseline_period in baseline_data:\n",
    "        baseline_psds.append(multitaper_psd(baseline_period))\n",
    "\n",
    "    # If no baseline periods, just leave it as 'False'\n",
    "    if len(baseline_psds):\n",
    "        f = baseline_psds[0][0]\n",
    "        Pxx = np.mean([psd[1] for psd in baseline_psds], axis=0)\n",
    "        return f, Pxx, baseline_data\n",
    "    else:\n",
    "        return False, False, False\n",
    "    \n",
    "def get_baseline_amplitude (data):\n",
    "    if data and len(data):\n",
    "        amps = []\n",
    "        for baseline_period in data:\n",
    "            baseline_period = butter_bandpass_filter(baseline_period, 4, 100)\n",
    "            amps.append(np.max(baseline_period)-np.min(baseline_period))\n",
    "        return np.mean(amps, axis=0) \n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def get_normalized_psd (bursts, baseline_psd):\n",
    "    normalized_psd_bursts = []\n",
    "\n",
    "    for burst in bursts:\n",
    "        # Scale PSDs to 0-100 Hz\n",
    "        f_baseline, Pxx_baseline = get_psd_in_range(baseline_psd, [0, 100])\n",
    "        f_burst, Pxx_burst = get_psd_in_range(multitaper_psd(burst.data), [0, 100])\n",
    "        \n",
    "        # Take ratio between burst and baseline power\n",
    "        Pxx_normed = Pxx_burst/Pxx_baseline\n",
    "\n",
    "        # Anything above 1 indicates greater power for burst over baseline\n",
    "        Pxx_clipped = np.clip(Pxx_normed, a_min=1, a_max=None)\n",
    "\n",
    "        # Find max peak (primary freq) in burst/baseline power ratio\n",
    "        peaks = signal.find_peaks(Pxx_clipped)[0]\n",
    "        if len(peaks):\n",
    "            max_power = max(Pxx_clipped[peaks])\n",
    "            max_power_idx = np.where(Pxx_clipped == max_power)[0]\n",
    "            max_freq = f_burst[max_power_idx][0]\n",
    "            burst.normalized_psd = (f_burst, Pxx_normed)\n",
    "            burst.primary_frequency_baseline = max_freq\n",
    "        else:\n",
    "            burst.primary_frequency_baseline = False\n",
    "        \n",
    "        normalized_psd_bursts.append(burst)\n",
    "        \n",
    "    return normalized_psd_bursts\n",
    "\n",
    "rms_processed_recordings = []\n",
    "for recording_idx, recording in enumerate(RECORDINGS):\n",
    "    print(\"P{} {}/{} {}\".format(recording[\"age\"], recording_idx+1, len(RECORDINGS), recording[\"path\"]))\n",
    "    \n",
    "    if recording[\"age\"] > 12:\n",
    "        continue\n",
    "    \n",
    "    for brain_area in [\"cortex\", \"thalamus\", \"striatum\"]:\n",
    "        brain_channel = brain_area + \"_channel\"\n",
    "        brain_bursts = brain_area + \"_bursts\"\n",
    "        brain_baseline = brain_area + \"_baseline\"\n",
    "        brain_baseline_amp = brain_area + \"_baseline_amplitude\"\n",
    "        \n",
    "        if not brain_channel in recording:\n",
    "            continue\n",
    "        else:\n",
    "            print('\\t{}'.format(brain_area))\n",
    "            \n",
    "        recording_n = recording[\"recording\"]\n",
    "        channel_n = recording[brain_channel]\n",
    "        session = Session(ROOT + recording[\"path\"])\n",
    "\n",
    "        data_all = session.recordings[recording_n].continuous[0].samples[get_slice_from_s(0, 60*60), channel_n]\n",
    "        data_all_4_to_100 = butter_bandpass_filter(data_all, 4, 100)\n",
    "        \n",
    "        hist, bin_edges, rms_list = rms_hist(data_all_4_to_100, window=0.2)\n",
    "        try:\n",
    "            use_truncated_hist = recording[\"path\"] != \"2021-02-11_15-57-01\" and recording[\"age\"] > 9 and recording[\"age\"] < 13\n",
    "            mu, sigma = fit_gaussian(hist, bin_edges, use_truncated_hist=use_truncated_hist)\n",
    "        except:\n",
    "            print('Warning! Could not fit Gaussian!')\n",
    "            continue\n",
    "        print('\\tComputed mean ({}) and sigma ({})'.format(mu, sigma))\n",
    "\n",
    "        bursts = get_burst_events(data_all_4_to_100, rms_list, mu, sigma, window=0.2)\n",
    "        print('\\tBursts found')\n",
    "        bursts = combine_adjacent_bursts(data_all_4_to_100, bursts, temporal_distance=0.2)\n",
    "        print('\\tAdjacent bursts combined')\n",
    "        \n",
    "        baseline_f, baseline_Pxx, baseline_data = get_baseline_power(data_all, bursts)\n",
    "        baseline_psd = baseline_f, baseline_Pxx\n",
    "        print('\\tBaseline periods found')\n",
    "        baseline_amplitude = get_baseline_amplitude(baseline_data)\n",
    "        print('\\tBaseline amplitudes found')\n",
    "        \n",
    "        bursts = filter_short_bursts(bursts, minimum_duration=0.2)\n",
    "        print('\\tFiltered short bursts')\n",
    "        bursts = filter_rms(bursts, max_rms=1000)\n",
    "        print('\\tFiltered RMS')\n",
    "        bursts = filter_min_peaks(bursts, min_peaks=5)\n",
    "        bursts = get_raw_bursts (data_all, bursts)\n",
    "        print('\\tGot 1500 Hz data')\n",
    "        \n",
    "        bursts = get_normalized_psd(bursts, baseline_psd)\n",
    "        print('\\tGot normed PSDs\\n')\n",
    "        \n",
    "        recording = recording.copy()\n",
    "        recording[brain_bursts] = bursts\n",
    "        recording[brain_baseline] = baseline_psd\n",
    "        recording[brain_baseline_amp] = baseline_amplitude\n",
    "        recording[\"length\"] = len(data_all)\n",
    "        \n",
    "    rms_processed_recordings.append(recording)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MUA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_MUA (data, standard_deviations=5, silent=False):\n",
    "    # Filter between 0.4-4kHz\n",
    "    data = butter_bandpass_filter(data, 400, 4000)\n",
    "    \n",
    "    # Produce threshold as anything below -5 std\n",
    "    threshold = standard_deviations*np.std(data)\n",
    "    \n",
    "    if not silent:\n",
    "        print('\\tCalculated MUA threshold ({})\\n'.format(round(threshold, 2)))\n",
    "    \n",
    "    # Find points below threshold\n",
    "    spike_times = np.where(data < -threshold)[0]\n",
    "    \n",
    "    # Several points in spike waveform might be below threshold\n",
    "    # So filter out contiguous (t and t+1) timepoints\n",
    "    spike_times_unique = []\n",
    "    prev_t = 0\n",
    "    for t_idx, t in enumerate(spike_times):        \n",
    "        if prev_t and (prev_t+1 != t):\n",
    "            spike_times_unique.append(prev_t)\n",
    "        if (t_idx+1) == len(spike_times) and (prev_t+1 != t):\n",
    "            spike_times_unique.append(t)\n",
    "        prev_t = t\n",
    "    \n",
    "    # How far to extract waveform in MUA signal from trough\n",
    "    size = int(SAMPLING_RATE * 1/1000 * 1) # 1 ms\n",
    "\n",
    "    # Extract waveforms and align to trough of waveform\n",
    "    spike_waveforms = []\n",
    "    for spike_t in spike_times_unique:\n",
    "        t_start = np.clip(spike_t - size, a_min=0, a_max=None)\n",
    "        t_end = np.clip(spike_t + size, a_min=None, a_max=len(data))\n",
    "        spike_waveform = data[t_start:t_end]\n",
    "        \n",
    "        trough_t = np.argmin(spike_waveform) + t_start\n",
    "        trough_t_start = np.clip(trough_t-size, a_min=0, a_max=None)\n",
    "        trough_t_end = np.clip(trough_t+size, a_min=None, a_max=len(data))\n",
    "        \n",
    "        spike_waveform_aligned = data[trough_t_start:trough_t_end]\n",
    "        if np.max(spike_waveform_aligned) > 0 and np.max(spike_waveform_aligned[:15]) > 0:       \n",
    "            spike_waveforms.append(spike_waveform_aligned)\n",
    "    \n",
    "    return spike_times_unique, spike_waveforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_recordings_mua = []\n",
    "\n",
    "for recording_idx, recording in enumerate(RECORDINGS):\n",
    "    print(\"P{} {}/{} {}\".format(recording[\"age\"], recording_idx+1, len(RECORDINGS), recording[\"path\"]))\n",
    "\n",
    "    for brain_area in [\"cortex\", \"striatum\", \"thalamus\"]:\n",
    "        brain_channel = brain_area + \"_channel\"\n",
    "        brain_mua = brain_area + \"_mua\"\n",
    "        \n",
    "        if not brain_channel in recording:\n",
    "            continue\n",
    "        else:\n",
    "            print('\\t{}'.format(brain_area))\n",
    "\n",
    "        recording_n = recording[\"recording\"]\n",
    "        channel_n = recording[brain_channel]\n",
    "        session = Session(ROOT + recording[\"path\"])\n",
    "\n",
    "        data_all = session.recordings[recording_n].continuous[0].samples[get_slice_from_s(0, 60*60), channel_n]\n",
    "        \n",
    "        recording[\"length\"] = len(data_all) / SAMPLING_RATE\n",
    "        recording[brain_mua] = detect_MUA(data_all)\n",
    "        \n",
    "    processed_recordings_mua.append(recording)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MUA activity across developmental time periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spikes_per_second = {\n",
    "    \"striatum\": {\n",
    "        \"5-6\": [],\n",
    "        \"7-8\": [],\n",
    "        \"9-10\": [],\n",
    "        \"11-12\": [],\n",
    "        \">12\": []\n",
    "    },\n",
    "    \"thalamus\": {\n",
    "        \"5-6\": [],\n",
    "        \"7-8\": [],\n",
    "        \"9-10\": [],\n",
    "        \"11-12\": [],\n",
    "        \">12\": []\n",
    "    },\n",
    "    \"cortex\": {\n",
    "        \"5-6\": [],\n",
    "        \"7-8\": [],\n",
    "        \"9-10\": [],\n",
    "        \"11-12\": [],\n",
    "        \">12\": []\n",
    "    }\n",
    "}\n",
    "\n",
    "spike_filled_time = {\n",
    "    \"striatum\": {\n",
    "        \"5-6\": [],\n",
    "        \"7-8\": [],\n",
    "        \"9-10\": [],\n",
    "        \"11-12\": [],\n",
    "        \">12\": []\n",
    "    },\n",
    "    \"thalamus\": {\n",
    "        \"5-6\": [],\n",
    "        \"7-8\": [],\n",
    "        \"9-10\": [],\n",
    "        \"11-12\": [],\n",
    "        \">12\": []\n",
    "    },\n",
    "    \"cortex\": {\n",
    "        \"5-6\": [],\n",
    "        \"7-8\": [],\n",
    "        \"9-10\": [],\n",
    "        \"11-12\": [],\n",
    "        \">12\": []\n",
    "    }\n",
    "}\n",
    "\n",
    "for brain_area in [\"striatum\", \"thalamus\", \"cortex\"]:\n",
    "    for recording in processed_recordings_mua_last:\n",
    "        if recording[\"age\"] < 7:\n",
    "            age = \"5-6\"\n",
    "        elif recording[\"age\"] >= 7 and recording[\"age\"] < 9:\n",
    "            age = \"7-8\"\n",
    "        elif recording[\"age\"] >= 9 and recording[\"age\"] < 11:\n",
    "            age = \"9-10\"\n",
    "        elif recording[\"age\"] >= 11 and recording[\"age\"] < 13:\n",
    "            age = \"11-12\"\n",
    "        else:\n",
    "            age = \">12\"\n",
    "            \n",
    "        if not brain_area + \"_mua\" in recording:\n",
    "            continue\n",
    "\n",
    "        sps = len(recording[brain_area + \"_mua\"][1]) / recording[\"length\"]\n",
    "        spikes_per_second[brain_area][age].append(sps)\n",
    "        \n",
    "        sparse_spike_times = recording[brain_area + \"_mua\"][0]\n",
    "        spike_times = np.zeros(int(recording[\"length\"]*SAMPLING_RATE))\n",
    "        spike_times[sparse_spike_times] = 1\n",
    "        \n",
    "        filled_time = 0\n",
    "        total_time = 0\n",
    "        \n",
    "        chunk_size = int(SAMPLING_RATE*1)\n",
    "        for i in range(0, len(spike_times), chunk_size):\n",
    "            spike_chunk = spike_times[i:i+chunk_size]\n",
    "            if 1 in spike_chunk:\n",
    "                filled_time += 1\n",
    "            total_time += 1\n",
    "            \n",
    "        sft = (filled_time / total_time) * 100\n",
    "        spike_filled_time[brain_area][age].append(sft)\n",
    "        \n",
    "print(twoway_anova(spikes_per_second, ['brain_area', 'age', 'spikes']))\n",
    "print(twoway_anova(spike_filled_time, ['brain_area', 'age', 'spikes']))\n",
    "\n",
    "plot_graph_by_age (\n",
    "    data=spikes_per_second,\n",
    "    ylabel='Spikes $\\mathregular{s^{-1}}$',\n",
    "    scatter=True,\n",
    "    colors=[COLOR_STRIATUM, COLOR_THALAMUS, COLOR_CORTEX],\n",
    "    labels=[\"Striatum\", 'CL/Pf', 'Cortex']\n",
    ")\n",
    "plot_graph_by_age (\n",
    "    data=spike_filled_time,\n",
    "    ylabel='Spike filled time(%)',\n",
    "    scatter=True,\n",
    "    colors=[COLOR_STRIATUM, COLOR_THALAMUS, COLOR_CORTEX],\n",
    "    labels=[\"Striatum\", 'CL/Pf', 'Cortex'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LFP and burst statistics over developmental time periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LFP_power = {\n",
    "    \"striatum\": {\n",
    "        \"5-6\": [],\n",
    "        \"7-8\": [],\n",
    "        \"9-10\": [],\n",
    "        \"11-12\": [],\n",
    "        \">12\": []\n",
    "    },\n",
    "    \"thalamus\": {\n",
    "        \"5-6\": [],\n",
    "        \"7-8\": [],\n",
    "        \"9-10\": [],\n",
    "        \"11-12\": [],\n",
    "        \">12\": []\n",
    "    },\n",
    "    \"cortex\": {\n",
    "        \"5-6\": [],\n",
    "        \"7-8\": [],\n",
    "        \"9-10\": [],\n",
    "        \"11-12\": [],\n",
    "        \">12\": []\n",
    "    }\n",
    "}   \n",
    "\n",
    "def bandpower(x, fs, fmin, fmax):\n",
    "    f, Pxx = scipy.signal.periodogram(x, fs=fs)\n",
    "    ind_min = scipy.argmax(f > fmin) - 1\n",
    "    ind_max = scipy.argmax(f > fmax) - 1\n",
    "    return scipy.trapz(Pxx[ind_min: ind_max], f[ind_min: ind_max])\n",
    "\n",
    "for recording_idx, recording in enumerate(RECORDINGS):\n",
    "    print(\"P{} {}/{} {}\".format(recording[\"age\"], recording_idx+1, len(RECORDINGS), recording[\"path\"]))\n",
    "    \n",
    "    if recording[\"age\"] < 7:\n",
    "        age = \"5-6\"\n",
    "    elif recording[\"age\"] >= 7 and recording[\"age\"] < 9:\n",
    "        age = \"7-8\"\n",
    "    elif recording[\"age\"] >= 9 and recording[\"age\"] < 11:\n",
    "        age = \"9-10\"\n",
    "    elif recording[\"age\"] >= 11 and recording[\"age\"] < 13:\n",
    "        age = \"11-12\"\n",
    "    else:\n",
    "        age = \">12\"\n",
    "\n",
    "    for brain_area in [\"striatum\", \"thalamus\", \"cortex\"]:\n",
    "        brain_channel = brain_area + \"_channel\"\n",
    "        \n",
    "        if not brain_channel in recording:\n",
    "            continue\n",
    "        else:\n",
    "            print('\\t{}'.format(brain_area))\n",
    "        \n",
    "        recording_n = recording[\"recording\"]\n",
    "        channel_n = recording[brain_channel]\n",
    "        session = Session(ROOT + recording[\"path\"])\n",
    "\n",
    "        data_all = session.recordings[recording_n].continuous[0].samples[get_slice_from_s(0, 60*10), channel_n]\n",
    "\n",
    "        lp = bandpower(data_all, SAMPLING_RATE, 4, 100)\n",
    "        \n",
    "        LFP_power[brain_area][age].append(lp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amplitude = {\n",
    "    \"striatum\": {\n",
    "        \"5-6\": [],\n",
    "        \"7-8\": [],\n",
    "        \"9-10\": [],\n",
    "        \"11-12\": [],\n",
    "        \">12\": []\n",
    "    },\n",
    "    \"thalamus\": {\n",
    "        \"5-6\": [],\n",
    "        \"7-8\": [],\n",
    "        \"9-10\": [],\n",
    "        \"11-12\": [],\n",
    "        \">12\": []\n",
    "    },\n",
    "    \"cortex\": {\n",
    "        \"5-6\": [],\n",
    "        \"7-8\": [],\n",
    "        \"9-10\": [],\n",
    "        \"11-12\": [],\n",
    "        \">12\": []\n",
    "    }\n",
    "}\n",
    "\n",
    "relative_amplitude = {\n",
    "    \"striatum\": {\n",
    "        \"5-6\": [],\n",
    "        \"7-8\": [],\n",
    "        \"9-10\": [],\n",
    "        \"11-12\": [],\n",
    "        \">12\": []\n",
    "    },\n",
    "    \"thalamus\": {\n",
    "        \"5-6\": [],\n",
    "        \"7-8\": [],\n",
    "        \"9-10\": [],\n",
    "        \"11-12\": [],\n",
    "        \">12\": []\n",
    "    },\n",
    "    \"cortex\": {\n",
    "        \"5-6\": [],\n",
    "        \"7-8\": [],\n",
    "        \"9-10\": [],\n",
    "        \"11-12\": [],\n",
    "        \">12\": []\n",
    "    }\n",
    "}\n",
    "\n",
    "occurence = {\n",
    "    \"striatum\": {\n",
    "        \"5-6\": [],\n",
    "        \"7-8\": [],\n",
    "        \"9-10\": [],\n",
    "        \"11-12\": [],\n",
    "        \">12\": []\n",
    "    },\n",
    "    \"thalamus\": {\n",
    "        \"5-6\": [],\n",
    "        \"7-8\": [],\n",
    "        \"9-10\": [],\n",
    "        \"11-12\": [],\n",
    "        \">12\": []\n",
    "    },\n",
    "    \"cortex\": {\n",
    "        \"5-6\": [],\n",
    "        \"7-8\": [],\n",
    "        \"9-10\": [],\n",
    "        \"11-12\": [],\n",
    "        \">12\": []\n",
    "    }\n",
    "}\n",
    "\n",
    "filled_time = {\n",
    "    \"striatum\": {\n",
    "        \"5-6\": [],\n",
    "        \"7-8\": [],\n",
    "        \"9-10\": [],\n",
    "        \"11-12\": [],\n",
    "        \">12\": []\n",
    "    },\n",
    "    \"thalamus\": {\n",
    "        \"5-6\": [],\n",
    "        \"7-8\": [],\n",
    "        \"9-10\": [],\n",
    "        \"11-12\": [],\n",
    "        \">12\": []\n",
    "    },\n",
    "    \"cortex\": {\n",
    "        \"5-6\": [],\n",
    "        \"7-8\": [],\n",
    "        \"9-10\": [],\n",
    "        \"11-12\": [],\n",
    "        \">12\": []\n",
    "    }\n",
    "}\n",
    "\n",
    "for brain_area in [\"striatum\", \"thalamus\", \"cortex\"]:\n",
    "    print(brain_area)\n",
    "    for idx, recording in enumerate(rms_processed_recordings):\n",
    "        if recording[\"age\"] < 7:\n",
    "            age = \"5-6\"\n",
    "        elif recording[\"age\"] >= 7 and recording[\"age\"] < 9:\n",
    "            age = \"7-8\"\n",
    "        elif recording[\"age\"] >= 9 and recording[\"age\"] < 11:\n",
    "            age = \"9-10\"\n",
    "        elif recording[\"age\"] >= 11 and recording[\"age\"] < 13:\n",
    "            age = \"11-12\"\n",
    "        else:\n",
    "            age = \">12\"\n",
    "\n",
    "        if not brain_area + \"_bursts\" in recording:\n",
    "            continue\n",
    "        else:\n",
    "            print('\\t{}/{}'.format(idx+1, len(rms_processed_recordings)))\n",
    "            \n",
    "        baseline = recording[brain_area + \"_baseline_amplitude\"]\n",
    "    \n",
    "        amp_arr = []\n",
    "        filled_time_arr = []    \n",
    "        \n",
    "        for burst in recording[brain_area + \"_bursts\"]:\n",
    "            data = butter_bandpass_filter(burst.data, 4, 100)\n",
    "            \n",
    "            dur = burst.time[1] - burst.time[0]\n",
    "            \n",
    "            if dur/SAMPLING_RATE > 20:\n",
    "                continue\n",
    "            \n",
    "            filled_time_arr.append(dur)\n",
    "            \n",
    "            amp = np.max(data) - np.min(data)\n",
    "            amp_arr.append(amp)\n",
    "            \n",
    "        if len(amp_arr):\n",
    "            amplitude[brain_area][age].append(np.mean(amp_arr))\n",
    "            relative_amplitude[brain_area][age].append(np.mean(amp_arr)/baseline)\n",
    "        \n",
    "        filled = (np.sum(filled_time_arr) / recording[\"length\"]) * 100\n",
    "        filled_time[brain_area][age].append(filled)\n",
    "        \n",
    "        occur = len(recording[brain_area + \"_bursts\"]) / (recording[\"length\"] / (60*SAMPLING_RATE))\n",
    "        occurence[brain_area][age].append(occur)\n",
    "        \n",
    "plot_graph_by_age (\n",
    "    data=amplitude,\n",
    "    ylabel='Burst amplitude (μV)',\n",
    "    scatter=True,\n",
    "    colors=[COLOR_STRIATUM, COLOR_THALAMUS, COLOR_CORTEX],\n",
    "    labels=['Striatum', 'CL/Pf', 'Cortex']\n",
    ") \n",
    "print(twoway_anova(amplitude, ['brain_area', 'age', 'amplitude']))\n",
    "\n",
    "plot_graph_by_age (\n",
    "    data=relative_amplitude,\n",
    "    ylabel='Relative burst amplitude ($\\mathregular{A/A_0}$)',\n",
    "    scatter=True,\n",
    "    colors=[COLOR_STRIATUM, COLOR_THALAMUS, COLOR_CORTEX],\n",
    "    labels=['Striatum', 'CL/Pf', 'Cortex']\n",
    ") \n",
    "print(twoway_anova(relative_amplitude, ['brain_area', 'age', 'amplitude']))\n",
    "\n",
    "plot_graph_by_age (\n",
    "    data=occurence,\n",
    "    ylabel='Bursts $\\mathregular{min^{-1}}$',\n",
    "    scatter=True,\n",
    "    colors=[COLOR_STRIATUM, COLOR_THALAMUS, COLOR_CORTEX],\n",
    "    labels=['Striatum', 'CL/Pf', 'Cortex']\n",
    ")\n",
    "print(twoway_anova(occurence, ['brain_area', 'age', 'occurence']))\n",
    "\n",
    "plot_graph_by_age (\n",
    "    data=filled_time,\n",
    "    ylabel='Burst filled time (%)',\n",
    "    scatter=True,\n",
    "    colors=[COLOR_STRIATUM, COLOR_THALAMUS, COLOR_CORTEX],\n",
    "    labels=['Striatum', 'CL/Pf', 'Cortex']\n",
    ")\n",
    "print(twoway_anova(filled_time, ['brain_area', 'age', 'filled_time']))\n",
    "\n",
    "\n",
    "plot_graph_by_age (\n",
    "    data=LFP_power,\n",
    "    ylabel='LFP power ($\\mathregular{μV^2}$)',\n",
    "    scatter=True,\n",
    "    colors=[COLOR_STRIATUM, COLOR_THALAMUS, COLOR_CORTEX],\n",
    "    labels=['Striatum', 'CL/Pf', 'Cortex']\n",
    ")\n",
    "print(twoway_anova(LFP_power, ['brain_area', 'age', 'power']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA and clustering procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rms (data):\n",
    "    square = [d**2 for d in data]\n",
    "    mean = np.mean(square)\n",
    "    root = mean**0.5\n",
    "    return root\n",
    "    \n",
    "def get_feature_vector (burst):\n",
    "    if not len(burst.data):\n",
    "        return False\n",
    "    \n",
    "    burst_data = butter_bandpass_filter(burst.data, 4, 100)\n",
    "    \n",
    "    # Duration\n",
    "    duration = (burst.time[1] - burst.time[0]) / SAMPLING_RATE\n",
    "    \n",
    "    # Negative peak\n",
    "    negative_peak = min(burst_data)\n",
    "    \n",
    "    # rms\n",
    "    rms_list = []\n",
    "    chunk_size = int(SAMPLING_RATE*0.2)\n",
    "    for i in range(0, len(burst_data), chunk_size):\n",
    "        data_chunk = burst_data[i:i+chunk_size]\n",
    "        rms_list.append(rms(data_chunk))\n",
    "    \n",
    "    # rms features\n",
    "    max_rms = np.max(rms_list)\n",
    "    min_rms = np.min(rms_list)\n",
    "    flatness = min_rms / max_rms\n",
    "    \n",
    "    # slope\n",
    "    data_downsampled = signal.resample(burst_data, int(len(burst_data)/SAMPLING_RATE * 500))\n",
    "    max_slope = np.max([x - z for x, z in zip(data_downsampled[:-1], data_downsampled[1:])])\n",
    "    \n",
    "    # Spectral power\n",
    "    f_burst, Pxx_burst = burst.normalized_psd\n",
    "    theta_idx = np.where(f_burst >= 4)[0][0] \n",
    "    beta_idx = np.where(f_burst >= 16)[0][0]\n",
    "    lgamma_idx = np.where(f_burst >= 40)[0][0]\n",
    "    \n",
    "    theta_power = np.sum(Pxx_burst[theta_idx:beta_idx]) / np.sum(Pxx_burst[theta_idx:])\n",
    "    beta_lgamma_power = np.sum(Pxx_burst[beta_idx:lgamma_idx]) / np.sum(Pxx_burst[theta_idx:])\n",
    "    \n",
    "    # Peak/trough features\n",
    "    peaks, cycles = get_peaks(burst_data)\n",
    "    iti = np.mean(np.diff([p for p in peaks if burst_data[p] < 0])) / SAMPLING_RATE\n",
    "    if np.isnan(iti):\n",
    "        return False\n",
    "    \n",
    "    return [\n",
    "        duration,\n",
    "        max_rms,\n",
    "        negative_peak,\n",
    "        flatness,\n",
    "        max_slope,\n",
    "        beta_lgamma_power,\n",
    "        theta_power,\n",
    "        iti\n",
    "    ]\n",
    "\n",
    "feature_vector_labels = [\n",
    "    \"Duration\",\n",
    "    \"Max RMS\",\n",
    "    \"Negative peak\",\n",
    "    \"Flatness\",\n",
    "    \"Max slope\",\n",
    "    \"β-γ power\",\n",
    "    \"θ-α power\",\n",
    "    \"ITI\",\n",
    "    \"Spike rate\"\n",
    "]\n",
    "feature_vector_labels_full = [\n",
    "    \"Duration (s)\",\n",
    "    \"Max RMS (μV)\",\n",
    "    \"Negative peak (μV)\",\n",
    "    \"Flatness\",\n",
    "    \"Max slope\",\n",
    "    \"Beta/low-gamma power\",\n",
    "    \"Theta-alpha power\",\n",
    "    \"Inter-trough-interval (s)\",\n",
    "    \"Spikes $s^{-1}$\"\n",
    "]\n",
    "\n",
    "all_bursts = []\n",
    "all_features = []\n",
    "\n",
    "for idx, recording in enumerate(rms_processed_recordings):\n",
    "    print(\"{} (P{}) {}/{}\".format(recording[\"path\"], recording[\"age\"], idx+1, len(rms_processed_recordings)))\n",
    "    \n",
    "    key = \"thalamus_bursts\" # or \"cortex_bursts\", \"striatum_bursts\"\n",
    "    if not key in recording:\n",
    "        continue\n",
    "\n",
    "    spike_times = processed_recordings_mua[idx][\"thalamus_mua\"][0]\n",
    "        \n",
    "    for burst_idx, burst in enumerate(recording[key]):\n",
    "        if (burst_idx+1) % 20 == 0:\n",
    "            print('\\t{}/{}'.format(burst_idx+1, len(recording[key])))\n",
    "\n",
    "        if burst.primary_frequency_baseline:\n",
    "            burst.feature_vec = get_feature_vector(burst)\n",
    "            if hasattr(burst, 'feature_vec') and burst.feature_vec and burst.feature_vec[0] < 20:\n",
    "                # Spikes in range of burst\n",
    "                spikes_in_range = [t for t in spike_times if (t >= burst.time[0] and t <= burst.time[1])]\n",
    "                spike_rate = len(spikes_in_range) / burst.feature_vec[0]\n",
    "                burst.feature_vec.append(spike_rate)\n",
    "                \n",
    "                burst.age = recording[\"age\"]\n",
    "                burst.recording_path = recording[\"path\"]\n",
    "\n",
    "                all_features.append(burst.feature_vec)\n",
    "                all_bursts.append(burst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fuzzy cluster with thresh\n",
    "def fuzzy_cluster (samples, n_clusters, threshold=0.6):\n",
    "    cntr, u, u0, d, jm, p, fpc = fuzz.cmeans(samples.T, n_clusters, 2, error=0.005, maxiter=10000, init=None)\n",
    "    \n",
    "    labels = []\n",
    "    for sample in u.T:\n",
    "        max_p = np.max(sample)\n",
    "        if max_p > threshold:\n",
    "            labels.append(np.argmax(sample, axis=0))\n",
    "        else:\n",
    "            labels.append(len(sample))\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    return labels, fpc\n",
    "    \n",
    "# Run PCA analysis using first 3 components\n",
    "pca = PCA(n_components=3)\n",
    "scaler = StandardScaler()\n",
    "pc_feature_list = pca.fit_transform(scaler.fit_transform(all_features))\n",
    "\n",
    "# Get optimal number of clusters\n",
    "n_clusters = np.arange(2, 6)\n",
    "silhouette_scores = []\n",
    "\n",
    "for n in n_clusters:\n",
    "    _, fpc = fuzzy_cluster(pc_feature_list, n)\n",
    "    silhouette_scores.append(fpc)\n",
    "\n",
    "plt.plot(n_clusters, silhouette_scores, c='black')\n",
    "plt.xticks(n_clusters)\n",
    "plt.xlabel('N clusters')\n",
    "plt.ylabel('Fuzzy partition coefficient')\n",
    "format_plot(plt.gca(), legend=False)\n",
    "plt.show()\n",
    "\n",
    "print('FPC values', silhouette_scores)\n",
    "optimal_n_clusters = n_clusters[np.argmax(silhouette_scores)]\n",
    "\n",
    "# Plot clusters (of first 3 components)\n",
    "cluster_data = {\n",
    "    \"cluster\": [],\n",
    "    \"pc1\": [],\n",
    "    \"pc2\": [],\n",
    "    \"pc3\": []\n",
    "}\n",
    "\n",
    "labels, _ = fuzzy_cluster(pc_feature_list, optimal_n_clusters)\n",
    "fig = plt.figure(figsize=[5, 5], dpi=150)\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "colors = [\"red\", \"blue\", \"gray\"]\n",
    "for cluster_i in [1,0,2]:\n",
    "    if cluster_i == 1:\n",
    "        label = 'NGB'\n",
    "        alpha=0.1\n",
    "    elif cluster_i == 0:\n",
    "        label = 'SB'\n",
    "        alpha=0.1\n",
    "    else:\n",
    "        label = 'UC'\n",
    "        alpha=0.5\n",
    "    \n",
    "    samples = pc_feature_list[labels == cluster_i]\n",
    "    ax.scatter(samples[:, 0], samples[:, 1], samples[:, 2], alpha=alpha, label=label, c=colors[cluster_i])\n",
    "    \n",
    "    for sample in samples:\n",
    "        cluster_data[\"cluster\"].append(label)\n",
    "        cluster_data[\"pc1\"].append(sample[0])\n",
    "        cluster_data[\"pc2\"].append(sample[1])\n",
    "        cluster_data[\"pc3\"].append(sample[1])\n",
    "    \n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_xlim([None,4.5])\n",
    "ax.set_ylabel('PC2')\n",
    "ax.set_ylim([-4.5,None])\n",
    "ax.set_zlabel('PC3', size=15)\n",
    "ax.set_zlim([None,8])\n",
    "ax.tick_params(axis='z', labelsize=12.5)\n",
    "format_plot(ax)\n",
    "plt.show()\n",
    "    \n",
    "# Visualize variance explained by each component\n",
    "total_features = len(feature_vector_labels)\n",
    "all_pca = PCA(n_components=total_features)\n",
    "all_pca.fit(scaler.fit_transform(all_features))\n",
    "plt.plot(np.cumsum(all_pca.explained_variance_ratio_), c='black')\n",
    "plt.xticks(np.arange(total_features), np.arange(total_features)+1)\n",
    "plt.xlabel('N components')\n",
    "plt.ylabel('Variance')\n",
    "format_plot(plt.gca(), legend=False)\n",
    "print('Explained variance', np.cumsum(all_pca.explained_variance_ratio_))\n",
    "\n",
    "\n",
    "# Visualize relative contribution of components\n",
    "component_contribution_data = {\n",
    "    \"component\": [],\n",
    "    \"duration\": [],\n",
    "    \"max_rms\": [],\n",
    "    \"negative_peak\": [],\n",
    "    \"flatness\": [],\n",
    "    \"max_slope\": [],\n",
    "    \"beta_lgamma_power\": [],\n",
    "    \"theta_power\": [],\n",
    "    \"iti\": [],\n",
    "    \"spike_rate\": []\n",
    "}\n",
    "component_contribution_keys = [k for k in component_contribution_data.keys()]\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=3, figsize=[15, 3.5])\n",
    "axs = list(axs.flat)\n",
    "for component_idx, component in enumerate(pca.components_):\n",
    "    ax = axs[component_idx]\n",
    "    xpos = np.arange(len(component))\n",
    "    \n",
    "    ax.bar(xpos, np.abs(component), facecolor='dimgray')\n",
    "    ax.set_xticks(xpos)\n",
    "    ax.set_xticklabels(feature_vector_labels, rotation=45)\n",
    "    ax.set_ylabel('Coefficient')\n",
    "    ax.set_title('PC{}'.format(component_idx+1))\n",
    "    ax.set_ylim([0, 0.8])\n",
    "    format_plot(ax, legend=False)\n",
    "    \n",
    "    component_contribution_data[\"component\"].append(component_idx+1)\n",
    "    for coeff_idx, coeff in enumerate(component):\n",
    "        component_contribution_data[component_contribution_keys[coeff_idx+1]].append(np.abs(coeff))\n",
    "    \n",
    "for ax in axs[total_features:]:\n",
    "    ax.set_visible(False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate F-ratio test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multivariate_f_ratio (a, b):\n",
    "    a_group_means = np.mean(a, axis=0)\n",
    "    b_group_means = np.mean(b, axis=0)\n",
    "    overall_means = np.mean(np.concatenate([a,b]), axis=0)\n",
    "\n",
    "    ss_between_group = 0\n",
    "    ss_between_group += (len(a) * np.linalg.norm(overall_means-a_group_means)**2)\n",
    "    ss_between_group += (len(b) * np.linalg.norm(overall_means-b_group_means)**2)\n",
    "    \n",
    "    ss_within_group = 0\n",
    "    for feature_vec in a:\n",
    "        ss_within_group += (np.linalg.norm(a_group_means-feature_vec)**2)\n",
    "    for feature_vec in b:\n",
    "        ss_within_group += (np.linalg.norm(b_group_means-feature_vec)**2)      \n",
    "    ss_within_group = ss_within_group / (len(a)+len(b) - 2)\n",
    "  \n",
    "    f = ss_between_group/ss_within_group\n",
    "    return f\n",
    "\n",
    "def permute_multivariate_f_ratio (a, b, iterations):\n",
    "    combined = np.concatenate([a, b])\n",
    "\n",
    "    test_stat = multivariate_f_ratio(a, b)\n",
    "    null_dist = []\n",
    "\n",
    "    for i in range(iterations):\n",
    "        shuffled = np.random.permutation(combined)\n",
    "        a_shuffled = shuffled[:len(a)]\n",
    "        b_shuffled = shuffled[len(a):]\n",
    "\n",
    "        f = multivariate_f_ratio(a_shuffled, b_shuffled)\n",
    "        null_dist.append(f)\n",
    "\n",
    "        if (i+1) % 1000 == 0:\n",
    "            print('Iteration', i+1)\n",
    "\n",
    "    p_val = len(np.where(null_dist>= test_stat)[0]) / len(null_dist)\n",
    "    \n",
    "    return p_val, test_stat, null_dist\n",
    "    \n",
    "# Lists of feature vectors for e.g., cortex ngb and thalamus ngb clusters\n",
    "# on which to perform F-ratio test\n",
    "burst_group_a = cortex_ngb_features\n",
    "burst_group_b = thalamus_ngb_features\n",
    "\n",
    "p, f, null = permute_multivariate_f_ratio(burst_group_a, burst_group_b, iterations=10000)\n",
    "\n",
    "bins = 100\n",
    "max_val = np.max(np.histogram(null, bins=bins)[0])\n",
    "\n",
    "plt.hist(null, bins=bins, facecolor='dimgray')\n",
    "plt.plot([f, f], [0, max_val])\n",
    "plt.xlabel('F-value')\n",
    "plt.ylabel('Count')\n",
    "format_plot(plt.gca(), legend=False)\n",
    "plt.show()\n",
    "print(\n",
    "    'p = {}, F({}, {}) = {}'.format(p, 1, len(burst_group_a) + len(burst_group_b) - 2, f)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developmental trajectory of burst properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def plot_group_lineplots (data, ylim, labels, xticks, colors=['blue', 'orange'], title='', xlabel='', ylabel='', yscale='linear', save=False):    \n",
    "    fig = plt.figure()\n",
    "    \n",
    "    lines = []\n",
    "    for idx, data_group in enumerate(data):\n",
    "        shift = None\n",
    "        \n",
    "        if len(data) == 1:\n",
    "            shift = 0\n",
    "        elif len(data) == 2 and idx == 0:\n",
    "            shift = -0.4\n",
    "        elif len(data) ==2 and idx == 1:\n",
    "            shift = 0.4\n",
    "\n",
    "        positions = np.array(range(len(data_group)))# * 2 + shift\n",
    "        line = plt.errorbar(\n",
    "            positions,\n",
    "            [np.mean(group) for group in data_group],\n",
    "            yerr=[np.std(group)/(len(data_group)**0.5) for group in data_group],\n",
    "            capsize=5,\n",
    "            c=colors[idx]\n",
    "        )\n",
    "        \n",
    "        for group_idx, group in enumerate(data_group):\n",
    "            xpos = [positions[group_idx] for i in range(len(group))]\n",
    "            plt.plot(xpos, group, c=colors[idx], lw=0, marker='o', alpha=0.25)\n",
    "        \n",
    "        lines.append(line)\n",
    "        \n",
    "    plt.xticks(np.arange(len(xticks)), xticks)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.yscale(yscale)\n",
    "    plt.ylim(ylim)\n",
    "        \n",
    "    leg = plt.legend([line for line in lines], labels, frameon=False, fontsize=15, bbox_to_anchor=(1, 1), loc='upper left')\n",
    "    for legobj in leg.legendHandles:\n",
    "        legobj.set_linewidth(2.0)\n",
    "        \n",
    "    ax = plt.gca()\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    \n",
    "    for item in ([ax.title] + ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "        item.set_fontsize(12.5)\n",
    "    for item in ([ax.xaxis.label, ax.yaxis.label] + ax.get_legend().get_texts()):\n",
    "        item.set_fontsize(15)\n",
    "    \n",
    "    if save:\n",
    "        plt.savefig(FIG_ROOT + save, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "# GROUP is group of bursts identified earlier via PCA\n",
    "GROUP = {\n",
    "    \"NGB\": cortex_ngb,\n",
    "    \"SB\": cortex_sb\n",
    "}\n",
    "    \n",
    "occurence = {\n",
    "    \"NGB\": {\n",
    "        \"5-6\": [],\n",
    "        \"7-8\": [],\n",
    "        \"9-10\": [],\n",
    "        \"11-12\": []\n",
    "    },\n",
    "    \"SB\": {\n",
    "        \"5-6\": [],\n",
    "        \"7-8\": [],\n",
    "        \"9-10\": [],\n",
    "        \"11-12\": []\n",
    "    }\n",
    "}\n",
    "\n",
    "amplitude = {\n",
    "    \"NGB\": {\n",
    "        \"5-6\": [],\n",
    "        \"7-8\": [],\n",
    "        \"9-10\": [],\n",
    "        \"11-12\": []\n",
    "    },\n",
    "    \"SB\": {\n",
    "        \"5-6\": [],\n",
    "        \"7-8\": [],\n",
    "        \"9-10\": [],\n",
    "        \"11-12\": []\n",
    "    }\n",
    "}\n",
    "\n",
    "duration = {\n",
    "    \"NGB\": {\n",
    "        \"5-6\": [],\n",
    "        \"7-8\": [],\n",
    "        \"9-10\": [],\n",
    "        \"11-12\": []\n",
    "    },\n",
    "    \"SB\": {\n",
    "        \"5-6\": [],\n",
    "        \"7-8\": [],\n",
    "        \"9-10\": [],\n",
    "        \"11-12\": []\n",
    "    }\n",
    "}\n",
    "\n",
    "spike_rate = {\n",
    "    \"NGB\": {\n",
    "        \"5-6\": [],\n",
    "        \"7-8\": [],\n",
    "        \"9-10\": [],\n",
    "        \"11-12\": []\n",
    "    },\n",
    "    \"SB\": {\n",
    "        \"5-6\": [],\n",
    "        \"7-8\": [],\n",
    "        \"9-10\": [],\n",
    "        \"11-12\": []\n",
    "    }\n",
    "}\n",
    "\n",
    "alphatheta_power = {\n",
    "    \"NGB\": {\n",
    "        \"5-6\": [],\n",
    "        \"7-8\": [],\n",
    "        \"9-10\": [],\n",
    "        \"11-12\": []\n",
    "    },\n",
    "    \"SB\": {\n",
    "        \"5-6\": [],\n",
    "        \"7-8\": [],\n",
    "        \"9-10\": [],\n",
    "        \"11-12\": []\n",
    "    }\n",
    "}\n",
    "betagamma_power = {\n",
    "    \"NGB\": {\n",
    "        \"5-6\": [],\n",
    "        \"7-8\": [],\n",
    "        \"9-10\": [],\n",
    "        \"11-12\": []\n",
    "    },\n",
    "    \"SB\": {\n",
    "        \"5-6\": [],\n",
    "        \"7-8\": [],\n",
    "        \"9-10\": [],\n",
    "        \"11-12\": []\n",
    "    }\n",
    "}\n",
    "\n",
    "alphatheta_peak = {\n",
    "    \"NGB\": {\n",
    "        \"5-6\": [],\n",
    "        \"7-8\": [],\n",
    "        \"9-10\": [],\n",
    "        \"11-12\": []\n",
    "    },\n",
    "    \"SB\": {\n",
    "        \"5-6\": [],\n",
    "        \"7-8\": [],\n",
    "        \"9-10\": [],\n",
    "        \"11-12\": []\n",
    "    }\n",
    "}\n",
    "betagamma_peak = {\n",
    "    \"NGB\": {\n",
    "        \"5-6\": [],\n",
    "        \"7-8\": [],\n",
    "        \"9-10\": [],\n",
    "        \"11-12\": []\n",
    "    },\n",
    "    \"SB\": {\n",
    "        \"5-6\": [],\n",
    "        \"7-8\": [],\n",
    "        \"9-10\": [],\n",
    "        \"11-12\": []\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "box_labels = [\"NGB\", \"SB\"]\n",
    "age_group_labels = list(occurence[\"SB\"].keys())\n",
    "\n",
    "for brain_area in [\"cortex\"]:\n",
    "    brain_area_bursts = brain_area + \"_bursts\"\n",
    "    \n",
    "    for recording_n, recording in enumerate(rms_processed_recordings):\n",
    "        if not brain_area_bursts in recording:\n",
    "            continue\n",
    "            \n",
    "        print(\"Processing {}, recording {}/{}\".format(brain_area, recording_n+1, len(rms_processed_recordings)))\n",
    "\n",
    "        age = recording[\"age\"]    \n",
    "        if age < 7:\n",
    "            age_group = \"5-6\"\n",
    "        elif age >= 7 and age < 9:\n",
    "            age_group = \"7-8\"\n",
    "        elif age >= 9 and age < 11:\n",
    "            age_group = \"9-10\"\n",
    "        elif age >= 11 and age < 13:\n",
    "            age_group = \"11-12\"\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        total_bursts_ngb = []\n",
    "        total_bursts_sb = []\n",
    "        \n",
    "        amplitude_recording = {\n",
    "            \"NGB\": [],\n",
    "            \"SB\": []\n",
    "        }\n",
    "        duration_recording = {\n",
    "            \"NGB\": [],\n",
    "            \"SB\": []\n",
    "        }\n",
    "        spike_rate_recording = {\n",
    "            \"NGB\": [],\n",
    "            \"SB\": []\n",
    "        }\n",
    "        alphatheta_recording = {\n",
    "            \"NGB\": [],\n",
    "            \"SB\": []\n",
    "        }\n",
    "        betagamma_recording = {\n",
    "            \"NGB\": [],\n",
    "            \"SB\": []\n",
    "        }\n",
    "        alphatheta_peak_recording = {\n",
    "            \"NGB\": [],\n",
    "            \"SB\": []\n",
    "        }\n",
    "        betagamma_peak_recording = {\n",
    "            \"NGB\": [],\n",
    "            \"SB\": []\n",
    "        }\n",
    "        \n",
    "        for burst_idx, burst in enumerate(recording[brain_area_bursts]):            \n",
    "            if (burst in GROUP['SB']):\n",
    "                group_key = 'SB'\n",
    "                total_bursts_sb.append(burst)\n",
    "            elif burst in GROUP['NGB']:\n",
    "                group_key = 'NGB'\n",
    "                total_bursts_ngb.append(burst)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            amplitude_recording[group_key].append(np.max(burst.data)-np.min(burst.data))\n",
    "            duration_recording[group_key].append((burst.time[1] - burst.time[0])/SAMPLING_RATE)\n",
    "            spike_rate_recording[group_key].append(burst.feature_vec[8])\n",
    "            alphatheta_recording[group_key].append(burst.feature_vec[6])\n",
    "            betagamma_recording[group_key].append(burst.feature_vec[5])\n",
    "            \n",
    "            f, Pxx = burst.normalized_psd\n",
    "\n",
    "            theta_idx = np.where(f >= 4)[0][0] \n",
    "            beta_idx = np.where(f >= 16)[0][0]\n",
    "            lgamma_idx = np.where(f >= 40)[0][0]\n",
    "\n",
    "            alphatheta_peak_ = f[np.argmax(Pxx[theta_idx:beta_idx])+theta_idx]\n",
    "            betagamma_peak_ = f[np.argmax(Pxx[beta_idx:lgamma_idx])+beta_idx]\n",
    "\n",
    "            alphatheta_peak_recording[group_key].append(alphatheta_peak_)\n",
    "            betagamma_peak_recording[group_key].append(betagamma_peak_)\n",
    "            \n",
    "        occurence_ngb = len(total_bursts_ngb) / (recording[\"length\"] / SAMPLING_RATE) * 60\n",
    "        occurence_sb = len(total_bursts_sb) / (recording[\"length\"] / SAMPLING_RATE) * 60\n",
    "        \n",
    "        occurence[\"NGB\"][age_group].append(occurence_ngb)\n",
    "        occurence[\"SB\"][age_group].append(occurence_sb)\n",
    "        \n",
    "        if len(amplitude_recording[\"NGB\"]):\n",
    "            amplitude[\"NGB\"][age_group].append(np.mean(amplitude_recording[\"NGB\"]))\n",
    "        if len(amplitude_recording[\"SB\"]):\n",
    "            amplitude[\"SB\"][age_group].append(np.nanmean(amplitude_recording[\"SB\"]))\n",
    "\n",
    "        if len(duration_recording[\"NGB\"]):\n",
    "            duration[\"NGB\"][age_group].append(np.mean(duration_recording[\"NGB\"]))\n",
    "        if len(duration_recording[\"SB\"]):\n",
    "            duration[\"SB\"][age_group].append(np.nanmean(duration_recording[\"SB\"]))\n",
    "            \n",
    "        if len(spike_rate_recording[\"NGB\"]):\n",
    "            spike_rate[\"NGB\"][age_group].append(np.mean(spike_rate_recording[\"NGB\"]))\n",
    "        if len(spike_rate_recording[\"SB\"]):\n",
    "            spike_rate[\"SB\"][age_group].append(np.mean(spike_rate_recording[\"SB\"]))\n",
    "        \n",
    "        if len(alphatheta_recording[\"NGB\"]):\n",
    "            alphatheta_power[\"NGB\"][age_group].append(np.mean(alphatheta_recording[\"NGB\"]))\n",
    "        if len(alphatheta_recording[\"SB\"]):\n",
    "            alphatheta_power[\"SB\"][age_group].append(np.mean(alphatheta_recording[\"SB\"]))\n",
    "        \n",
    "        if len(betagamma_recording[\"NGB\"]):\n",
    "            betagamma_power[\"NGB\"][age_group].append(np.mean(betagamma_recording[\"NGB\"]))\n",
    "        if len(betagamma_recording[\"SB\"]):\n",
    "            betagamma_power[\"SB\"][age_group].append(np.mean(betagamma_recording[\"SB\"]))\n",
    "        \n",
    "        if len(alphatheta_peak_recording[\"NGB\"]):\n",
    "            alphatheta_peak[\"NGB\"][age_group].append(np.mean(alphatheta_peak_recording[\"NGB\"]))\n",
    "        if len(alphatheta_peak_recording[\"SB\"]):\n",
    "            alphatheta_peak[\"SB\"][age_group].append(np.mean(alphatheta_peak_recording[\"SB\"]))\n",
    "        \n",
    "        if len(betagamma_peak_recording[\"NGB\"]):\n",
    "            betagamma_peak[\"NGB\"][age_group].append(np.mean(betagamma_peak_recording[\"NGB\"]))\n",
    "        if len(betagamma_peak_recording[\"SB\"]):\n",
    "            betagamma_peak[\"SB\"][age_group].append(np.mean(betagamma_peak_recording[\"SB\"]))\n",
    "        \n",
    "# Parameters to pass to line plot function\n",
    "plot_parameters = [\n",
    "    { \"title\": \"occurence\", \"ylabel\": \"Bursts $\\mathregular{s^{-1}}$\", \"data\": occurence, \"ylim\": [0, 0.16] },\n",
    "    { \"title\": \"amplitude\", \"ylabel\": \"Amplitude (μV)\", \"data\": amplitude, \"ylim\": [200, 2000] },\n",
    "    { \"title\": \"duration\", \"ylabel\": \"Duration (s)\", \"data\": duration, \"ylim\": [0, 10] },\n",
    "    { \"title\": \"spike_rate\", \"ylabel\": \"Spikes $\\mathregular{s^{-1}}$\", \"data\": spike_rate, \"ylim\": [0, 32] },\n",
    "    { \"title\": \"alphatheta\", \"ylabel\": \"Relative alpha-theta power\", \"data\": alphatheta_power, \"ylim\": [0, 0.6] },\n",
    "    { \"title\": \"betagamma\", \"ylabel\": \"Relative beta-gamma power\", \"data\": betagamma_power, \"ylim\": [0.25, 0.65] },\n",
    "    { \"title\": \"alphatheta_peak\", \"ylabel\": \"Peak alpha-theta frequency\", \"data\": alphatheta_peak, \"ylim\": [5, 15] },\n",
    "    { \"title\": \"betagamma_peak\", \"ylabel\": \"Peak beta-gamma frequency\", \"data\": betagamma_peak, \"ylim\": [15, 30] }\n",
    "]\n",
    "\n",
    "# Line plots\n",
    "for plot_param in plot_parameters:\n",
    "    data = plot_param[\"data\"]\n",
    "    age_groups_data = []\n",
    "    \n",
    "    print(plot_param[\"title\"])\n",
    "\n",
    "    for brain_area_values in zip(data.keys(), data.values()):\n",
    "        brain_key, brain_area = brain_area_values\n",
    "        print('\\t', brain_key)\n",
    "        age_groups = []\n",
    "        for age_group_values in zip(brain_area.keys(), brain_area.values()):\n",
    "            age_group_key, age_group = age_group_values\n",
    "            print('\\t {} ({})'.format(round(np.mean(age_group), 1), round(np.std(age_group), 2)))\n",
    "            age_groups.append(age_group)\n",
    "        age_groups_data.append(age_groups)\n",
    "\n",
    "    plot_group_lineplots(\n",
    "        data=age_groups_data,\n",
    "        ylim=plot_param[\"ylim\"],\n",
    "        labels=box_labels,\n",
    "        xticks=age_group_labels,\n",
    "        colors=['tab:blue', 'tab:red'],\n",
    "        xlabel=\"Age (days)\",\n",
    "        ylabel=plot_param[\"ylabel\"],\n",
    "        yscale=('log' if plot_param[\"title\"] == \"Duration\" or plot_param[\"title\"] == \"Amplitude\" else 'linear')\n",
    "    )\n",
    "    \n",
    "    print(twoway_anova(data, ['burst_type', 'age', 'value']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Burst synchrony\n",
    "### General functions\n",
    "Used for getting co-occuring bursts across brain regions and their lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_overlapping_bursts (bursts_a, bursts_b, bursts_c=None):\n",
    "    # maximum difference in start times considered overlapping\n",
    "    max_t_diff = 0.5\n",
    "    burst_pairs = []\n",
    "\n",
    "    for burst_a in bursts_a:\n",
    "        start_t_a = burst_a.time[0]\n",
    "\n",
    "        for burst_b in bursts_b:\n",
    "            start_t_b = burst_b.time[0]\n",
    "            \n",
    "            if bursts_c:\n",
    "                for burst_c in bursts_c:\n",
    "                    start_t_c = burst_c.time[0]\n",
    "                    \n",
    "                    t_diff_ab = abs(start_t_a - start_t_b)\n",
    "                    t_diff_ac = abs(start_t_a - start_t_c)\n",
    "                    t_diff_bc = abs(start_t_b - start_t_c)\n",
    "                    \n",
    "                    max_diff = max(t_diff_ab, t_diff_ac, t_diff_bc)\n",
    "                    min_len = min(len(burst_a.data), len(burst_b.data), len(burst_c.data))\n",
    "                    \n",
    "                    if max_diff < (SAMPLING_RATE*max_t_diff) and max_diff < min_len:\n",
    "                        burst_pairs.append(\n",
    "                            { \"bursts\": [burst_a, burst_b, burst_c], \"hash\": hash(burst_a)+hash(burst_b)+hash(burst_c) }\n",
    "                        )\n",
    "            else:\n",
    "                t_diff = abs(start_t_a - start_t_b)\n",
    "                if t_diff < (SAMPLING_RATE*max_t_diff) and t_diff < min(len(burst_a.data), len(burst_b.data)):\n",
    "                    burst_pairs.append(\n",
    "                        { \"bursts\": [burst_a, burst_b], \"hash\": hash(burst_a)+hash(burst_b) }\n",
    "                    )\n",
    "    \n",
    "    burst_pairs = np.array(burst_pairs)\n",
    "    unique_burst_pair_idxs = np.unique([pair[\"hash\"] for pair in burst_pairs], return_index=True)[1]\n",
    "    unique_burst_pairs = burst_pairs[unique_burst_pair_idxs]\n",
    "    unique_burst_pairs = [pair[\"bursts\"] for pair in burst_pairs]\n",
    "    \n",
    "    return unique_burst_pairs\n",
    "\n",
    "def get_nonoverlapping_bursts (bursts_a, bursts_b, burst_pairs):\n",
    "    bursts_a_nonoverlapping = []\n",
    "    for burst_a in bursts_a:\n",
    "        is_overlapping = False\n",
    "        for burst_pair in burst_pairs:\n",
    "            if burst_a in burst_pair:\n",
    "                is_overlapping = True\n",
    "                break\n",
    "        if not is_overlapping:\n",
    "            bursts_a_nonoverlapping.append(burst_a)\n",
    "\n",
    "    bursts_b_nonoverlapping = []\n",
    "    for burst_b in bursts_b:\n",
    "        is_overlapping = False\n",
    "        for burst_pair in burst_pairs:\n",
    "            if burst_b in burst_pair:\n",
    "                is_overlapping = True\n",
    "                break\n",
    "        if not is_overlapping:\n",
    "            bursts_b_nonoverlapping.append(burst_b)\n",
    "      \n",
    "    return bursts_a_nonoverlapping, bursts_b_nonoverlapping\n",
    "\n",
    "def align_overlapping_bursts (burst_a, burst_b):\n",
    "    start_overlap = max(burst_a.time[0], burst_b.time[0])\n",
    "    end_overlap = min(burst_a.time[1], burst_b.time[1])\n",
    "    \n",
    "    \n",
    "    start_a = start_overlap-burst_a.time[0]\n",
    "    end_a = start_a + (end_overlap-start_overlap)\n",
    "    start_b = start_overlap-burst_b.time[0]\n",
    "    end_b = start_b + (end_overlap-start_overlap)\n",
    "    \n",
    "    burst_a_trimmed = burst_a.data[start_a:end_a]\n",
    "    burst_b_trimmed = burst_b.data[start_b:end_b]\n",
    "    \n",
    "    return (\n",
    "        burst_a_trimmed,\n",
    "        burst_b_trimmed,\n",
    "        get_xticks(slice(start_overlap, end_overlap))\n",
    "    )\n",
    "\n",
    "def get_threshold (burst_pairs):\n",
    "    f_arr = None\n",
    "    Cxy_arr = []\n",
    "    \n",
    "    a_burst, b_burst = burst_pairs[0]\n",
    "    a, b, xticks = align_overlapping_bursts(a_burst, b_burst)\n",
    "    for iteration in range(1000):\n",
    "        np.random.shuffle(a)\n",
    "        np.random.shuffle(b)\n",
    "\n",
    "        window_size = 0.5\n",
    "        window = int(SAMPLING_RATE*window_size)\n",
    "        overlap = int(SAMPLING_RATE*window_size*0.5)\n",
    "        f, Cxy = signal.coherence(a, b, fs=SAMPLING_RATE, nperseg=window, noverlap=overlap)\n",
    "        f, Cxy = get_psd_in_range((f, Cxy), [0, 100])\n",
    "        \n",
    "        plt.plot(a)\n",
    "        plt.plot(b)\n",
    "        plt.show()\n",
    "\n",
    "        f_arr = f\n",
    "        Cxy_arr.append(Cxy)\n",
    "        \n",
    "    Cxy_arr = np.array(Cxy_arr)\n",
    "\n",
    "    return np.percentile(Cxy_arr, 95, axis=0)\n",
    "\n",
    "def get_lags (burst_a_data, burst_b_data, freq, prewhiten):\n",
    "    burst_a_data = butter_bandpass_filter(burst_a_data, freq[0], freq[1])\n",
    "    burst_b_data = butter_bandpass_filter(burst_b_data, freq[0], freq[1])\n",
    "    \n",
    "    # Get instantaneous amplitude\n",
    "    analytic_signal_a = signal.hilbert(burst_a_data)\n",
    "    amplitude_a = np.abs(analytic_signal_a)\n",
    "    analytic_signal_b = signal.hilbert(burst_b_data)\n",
    "    amplitude_b = np.abs(analytic_signal_b)\n",
    "    \n",
    "    # Remove dc component\n",
    "    amplitude_a = amplitude_a - np.mean(amplitude_a)\n",
    "    amplitude_b = amplitude_b - np.mean(amplitude_b)\n",
    "\n",
    "    # Pre-whiten\n",
    "    if prewhiten:\n",
    "        model = ARIMA(amplitude_a, order=(0,1,1))\n",
    "        model_fit=model.fit()\n",
    "        resid_a = model_fit.resid\n",
    "        \n",
    "        a = resid_a\n",
    "    else:\n",
    "        a = amplitude_a\n",
    "\n",
    "    # Normalize cross-correlation\n",
    "    resid_a = (a - np.mean(a)) / (np.std(a) * len(a))\n",
    "    amplitude_b = (amplitude_b - np.mean(amplitude_b)) / (np.std(amplitude_b))\n",
    "    cross_corr = signal.correlate(a, amplitude_b)\n",
    "    \n",
    "    # Take only +/- 100ms from 0 lag\n",
    "    centre = int(len(cross_corr) * 0.5)\n",
    "    hundred_ms = int(SAMPLING_RATE*0.1)\n",
    "    cross_corr = cross_corr[centre-hundred_ms:centre+hundred_ms]\n",
    "\n",
    "    # Square results\n",
    "    cross_corr_sq = cross_corr**2\n",
    "    \n",
    "    # Convert time points into ms\n",
    "    xpos = np.linspace(-hundred_ms, hundred_ms, len(cross_corr))\n",
    "    xpos = xpos / SAMPLING_RATE * 1000\n",
    "    \n",
    "    return xpos, cross_corr_sq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get proportion of synchronous events by age and region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cooccuring_events = {\n",
    "    \"Thal-CP\": [],\n",
    "    \"Ctx-CP\": [],\n",
    "    \"Ctx-thal\": [],\n",
    "    \"Ctx-thal-CP\": []\n",
    "}\n",
    "\n",
    "cooccuring_events_by_age = {\n",
    "    \"Thal-CP\": {\n",
    "        \"5-6\": [],\n",
    "        \"7-8\": [],\n",
    "        \"9-10\": [],\n",
    "        \"11-12\": [],\n",
    "        \">12\": []\n",
    "    },\n",
    "    \"Ctx-CP\": {\n",
    "        \"5-6\": [],\n",
    "        \"7-8\": [],\n",
    "        \"9-10\": [],\n",
    "        \"11-12\": [],\n",
    "        \">12\": []\n",
    "    },\n",
    "    \"Ctx-thal\": {\n",
    "        \"5-6\": [],\n",
    "        \"7-8\": [],\n",
    "        \"9-10\": [],\n",
    "        \"11-12\": [],\n",
    "        \">12\": []\n",
    "    },\n",
    "    \"Ctx-thal-CP\": {\n",
    "        \"5-6\": [],\n",
    "        \"7-8\": [],\n",
    "        \"9-10\": [],\n",
    "        \"11-12\": [],\n",
    "        \">12\": []\n",
    "    }\n",
    "}\n",
    "\n",
    "nonoverlapping_events = {\n",
    "    \"thalamus\": [],\n",
    "    \"striatum\": [],\n",
    "    \"cortex\": []\n",
    "}\n",
    "\n",
    "proportion_cooccuring_events = {\n",
    "    \"Thal-CP\": {\n",
    "        \"5-6\": [],\n",
    "        \"7-8\": [],\n",
    "        \"9-10\": [],\n",
    "        \"11-12\": [],\n",
    "        \">12\": []\n",
    "    },\n",
    "    \"Ctx-CP\": {\n",
    "        \"5-6\": [],\n",
    "        \"7-8\": [],\n",
    "        \"9-10\": [],\n",
    "        \"11-12\": [],\n",
    "        \">12\": []\n",
    "    },\n",
    "    \"Ctx-thal\": {\n",
    "        \"5-6\": [],\n",
    "        \"7-8\": [],\n",
    "        \"9-10\": [],\n",
    "        \"11-12\": [],\n",
    "        \">12\": []\n",
    "    },\n",
    "    \"Ctx-thal-CP\": {\n",
    "        \"5-6\": [],\n",
    "        \"7-8\": [],\n",
    "        \"9-10\": [],\n",
    "        \"11-12\": [],\n",
    "        \">12\": []\n",
    "    }\n",
    "}\n",
    "\n",
    "for recording_idx, recording in enumerate(rms_processed_recordings):\n",
    "    if recording[\"age\"] < 7:\n",
    "        age = \"5-6\"\n",
    "    elif recording[\"age\"] >= 7 and recording[\"age\"] < 9:\n",
    "        age = \"7-8\"\n",
    "    elif recording[\"age\"] >= 9 and recording[\"age\"] < 11:\n",
    "        age = \"9-10\"\n",
    "    elif recording[\"age\"] >= 11 and recording[\"age\"] < 13:\n",
    "        age = \"11-12\"\n",
    "    else:\n",
    "        age = \">12\"\n",
    "\n",
    "    \n",
    "    Ctx_CP = get_overlapping_bursts (recording[\"cortex_bursts\"], recording[\"striatum_bursts\"])\n",
    "    Ctx_nonoverlap, CP_nonoverlap = get_nonoverlapping_bursts (recording[\"cortex_bursts\"], recording[\"striatum_bursts\"], Ctx_CP)\n",
    "    nonoverlapping_events[\"cortex\"] += Ctx_nonoverlap\n",
    "    nonoverlapping_events[\"striatum\"] += CP_nonoverlap\n",
    "    cooccuring_events[\"Ctx-CP\"] += Ctx_CP\n",
    "    cooccuring_events_by_age[\"Ctx-CP\"][age] += Ctx_CP\n",
    "    proportion_cooccuring_events[\"Ctx-CP\"][age].append(\n",
    "        len(Ctx_CP) / (len(recording[\"cortex_bursts\"]) + len(recording[\"striatum_bursts\"]))\n",
    "    )\n",
    "    \n",
    "    if \"thalamus_bursts\" in recording:\n",
    "        Thal_CP = get_overlapping_bursts (recording[\"thalamus_bursts\"], recording[\"striatum_bursts\"])\n",
    "        thal_nonoverlap, cp_nonoverlap = get_nonoverlapping_bursts (recording[\"thalamus_bursts\"], recording[\"striatum_bursts\"], Thal_CP)\n",
    "        nonoverlapping_events[\"thalamus\"] += thal_nonoverlap\n",
    "        nonoverlapping_events[\"striatum\"] += cp_nonoverlap\n",
    "        cooccuring_events[\"Thal-CP\"] += Thal_CP\n",
    "        cooccuring_events_by_age[\"Thal-CP\"][age] += Thal_CP\n",
    "        proportion_cooccuring_events[\"Thal-CP\"][age].append(\n",
    "            len(Thal_CP) / (len(recording[\"thalamus_bursts\"]) + len(recording[\"striatum_bursts\"]))\n",
    "        )\n",
    "        \n",
    "        Ctx_thal = get_overlapping_bursts (recording[\"cortex_bursts\"], recording[\"thalamus_bursts\"])\n",
    "        ctx_nonoverlap, thal_nonoverlap = get_nonoverlapping_bursts (recording[\"cortex_bursts\"], recording[\"thalamus_bursts\"], Ctx_thal)\n",
    "        nonoverlapping_events[\"cortex\"] += ctx_nonoverlap\n",
    "        nonoverlapping_events[\"thalamus\"] += thal_nonoverlap\n",
    "        cooccuring_events[\"Ctx-thal\"] += Ctx_thal\n",
    "        cooccuring_events_by_age[\"Ctx-thal\"][age] += Ctx_thal\n",
    "        proportion_cooccuring_events[\"Ctx-thal\"][age].append(\n",
    "            len(Ctx_thal) / (len(recording[\"cortex_bursts\"]) + len(recording[\"thalamus_bursts\"]))\n",
    "        )\n",
    "        \n",
    "        Ctx_thal_CP = get_overlapping_bursts (recording[\"cortex_bursts\"], recording[\"thalamus_bursts\"], recording[\"striatum_bursts\"])\n",
    "        cooccuring_events[\"Ctx-thal-CP\"] += Ctx_thal_CP\n",
    "        cooccuring_events_by_age[\"Ctx-thal-CP\"][age] += Ctx_thal_CP\n",
    "        proportion_cooccuring_events[\"Ctx-thal-CP\"][age].append(\n",
    "            len(Ctx_thal_CP) / (len(recording[\"cortex_bursts\"]) + len(recording[\"thalamus_bursts\"]) + len(recording[\"striatum_bursts\"]))\n",
    "        )\n",
    "\n",
    "    print(\"{}/{}\".format(recording_idx+1, len(rms_processed_recordings)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot previous results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate proportions (over all ages)   \n",
    "bar_means = []\n",
    "bar_stderrs = []\n",
    "bar_labels = []\n",
    "    \n",
    "for comparison in proportion_cooccuring_events.keys():\n",
    "    data = sum([val for val in proportion_cooccuring_events[comparison].values()], [])\n",
    "    mean = np.mean(data)\n",
    "    stderr = np.std(data) / len(data)\n",
    "    \n",
    "    bar_means.append(mean)\n",
    "    bar_stderrs.append(stderr)\n",
    "    bar_labels.append(comparison)\n",
    "xpos = np.arange(len(bar_means))    \n",
    "        \n",
    "plt.bar(xpos, bar_means, yerr=bar_stderrs, capsize=5)\n",
    "plt.xticks(xpos, bar_labels)\n",
    "plt.ylabel('Proportion of total events')\n",
    "plt.title('Proportion of co-occuring events')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "ngb_pairs = []\n",
    "sb_pairs = []\n",
    "mixed_pairs = []\n",
    "\n",
    "for recording in rms_processed_recordings:\n",
    "    if not \"thalamus_bursts\" in recording:\n",
    "        continue\n",
    "    \n",
    "    recording_ngb = []\n",
    "    recording_sb = []\n",
    "    \n",
    "    recording_ngb_pairs = []\n",
    "    recording_sb_pairs = []\n",
    "    recording_mixed_pairs = []\n",
    "\n",
    "    # By burst type\n",
    "    for burst_pair in cooccuring_events[\"Thal-CP\"]:\n",
    "        if not burst_pair[0] in recording[\"thalamus_bursts\"]:\n",
    "            continue\n",
    "            \n",
    "        thal_burst, cp_burst = burst_pair\n",
    "        if thal_burst in thalamus_ngb and cp_burst in striatum_ngb:\n",
    "            recording_ngb_pairs.append(burst_pair)\n",
    "            recording_ngb.append(thal_burst)\n",
    "            recording_ngb.append(cp_burst)\n",
    "        elif thal_burst in thalamus_sb and cp_burst in striatum_sb:\n",
    "            recording_sb_pairs.append(burst_pair)\n",
    "            recording_sb.append(thal_burst)\n",
    "            recording_sb.append(cp_burst)\n",
    "        elif thal_burst in thalamus_ngb and cp_burst in striatum_sb:\n",
    "            recording_mixed_pairs.append(burst_pair)\n",
    "            recording_ngb.append(thal_burst)\n",
    "            recording_sb.append(cp_burst)\n",
    "        elif thal_burst in thalamus_sb and cp_burst in striatum_ngb:\n",
    "            recording_mixed_pairs.append(burst_pair) \n",
    "            recording_sb.append(thal_burst)\n",
    "            recording_ngb.append(cp_burst)\n",
    "            \n",
    "    if len(recording_ngb_pairs):\n",
    "        ngb_pairs.append( len(recording_ngb_pairs) / len(recording_ngb) )\n",
    "    if len(recording_sb_pairs):\n",
    "        sb_pairs.append( len(recording_sb_pairs) / len(recording_sb) )\n",
    "    if len(recording_mixed_pairs):\n",
    "        mixed_pairs.append( len(recording_mixed_pairs) / (len(recording_ngb)+len(recording_sb)) )\n",
    "\n",
    "xpos = np.arange(3)\n",
    "yvals = [np.mean(ngb_pairs), np.mean(sb_pairs), np.mean(mixed_pairs)]\n",
    "yerrs = [\n",
    "    np.std(ngb_pairs)/len(ngb_pairs)**0.5,\n",
    "    np.std(sb_pairs)/len(sb_pairs)**0.5,\n",
    "    np.std(mixed_pairs)/len(mixed_pairs)**0.5\n",
    "]\n",
    "\n",
    "labels = [\"Both NGB\", \"Both SB\", \"SB and NGB\"]\n",
    "colors = [\"tab:blue\", \"tab:red\", 'dimgray']\n",
    "bars = plt.bar(xpos, yvals, yerr=yerrs, capsize=5)\n",
    "for idx, bar_i in enumerate(bars):\n",
    "    bar_i.set_color(colors[idx])\n",
    "plt.xticks(xpos, labels)\n",
    "plt.ylabel('Normalized frequency')\n",
    "format_plot(plt.gca(), legend=False)\n",
    "plt.show()\n",
    "\n",
    "print('Mean =', yvals)\n",
    "print('Std =', [np.std(ngb_pairs), np.std(sb_pairs), np.std(mixed_pairs)])\n",
    "print('NGB vs SB', stats.ttest_ind(ngb_pairs, sb_pairs, equal_var=False))\n",
    "print('Cohen\\'s d =', cohen_d_for_welch(ngb_pairs, sb_pairs))\n",
    "print('DOF = ', welch_dof(ngb_pairs, sb_pairs))\n",
    "\n",
    "print('\\nNGB vs mixed', stats.ttest_ind(ngb_pairs, mixed_pairs, equal_var=False))\n",
    "print('Cohen\\'s d =', cohen_d_for_welch(ngb_pairs, mixed_pairs))\n",
    "print('DOF = ', welch_dof(ngb_pairs, mixed_pairs))\n",
    "\n",
    "print('\\nSB vs mixed', stats.ttest_ind(sb_pairs, mixed_pairs, equal_var=False))\n",
    "print('Cohen\\'s d =', cohen_d_for_welch(sb_pairs, mixed_pairs))\n",
    "print('DOF = ', welch_dof(sb_pairs, mixed_pairs))\n",
    "    \n",
    "# By age\n",
    "for comparison in proportion_cooccuring_events.keys():\n",
    "    data_by_age = proportion_cooccuring_events[comparison]\n",
    "    \n",
    "    labels = []\n",
    "    means = []\n",
    "    stderrs = []\n",
    "    \n",
    "    for data, label in zip(data_by_age.values(), data_by_age.keys()):\n",
    "        if len(data):\n",
    "            means.append(np.mean(data))\n",
    "            stderrs.append(np.std(data) / len(data)**0.5)\n",
    "            labels.append(label)\n",
    "          \n",
    "    xpos = np.arange(len(labels))\n",
    "    means = np.array(means)\n",
    "    stderrs = np.array(stderrs)\n",
    "        \n",
    "    plt.plot(xpos, means, label=comparison)\n",
    "    plt.fill_between(xpos, means-stderrs, means+stderrs, alpha=0.25)\n",
    "    plt.xticks(xpos, labels)\n",
    "    \n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Proportion of total events')\n",
    "plt.title('Proportion of co-occuring events')\n",
    "plt.legend(bbox_to_anchor=(1,1), loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-spectral coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 0.5\n",
    "window = int(SAMPLING_RATE*window_size)\n",
    "overlap = int(SAMPLING_RATE*window_size*0)\n",
    "\n",
    "Cxy_arr_shuffled_ngb = []\n",
    "Cxy_arr_ngb = []\n",
    "\n",
    "Cxy_arr_shuffled_sb = []\n",
    "Cxy_arr_sb = []\n",
    "\n",
    "mean_coherence_ngb = []\n",
    "mean_coherence_shuffled_ngb = []\n",
    "\n",
    "mean_coherence_sb = []\n",
    "mean_coherence_shuffled_sb = []\n",
    "\n",
    "bursts_a_ngb = []\n",
    "bursts_b_ngb = []\n",
    "\n",
    "bursts_a_sb = []\n",
    "bursts_b_sb = []\n",
    "\n",
    "for burst_pair_idx, burst_pair in enumerate(cooccuring_events[\"Ctx-CP\"]):\n",
    "    burst_a, burst_b = burst_pair\n",
    "    a, b, ticks = align_overlapping_bursts (burst_a, burst_b)\n",
    "    \n",
    "    if (burst_pair_idx+1) % 100 == 0:\n",
    "        print('Burst pair', burst_pair_idx+1)\n",
    "\n",
    "    a = butter_bandpass_filter(a, 4, 80)\n",
    "    b = butter_bandpass_filter(b, 4, 80)\n",
    "\n",
    "    if len(a)/SAMPLING_RATE < 0.5:\n",
    "        continue\n",
    "\n",
    "    f, Cxy = signal.coherence(a, b, fs=SAMPLING_RATE, nperseg=window, noverlap=overlap)\n",
    "    f, Cxy = get_psd_in_range((f, Cxy), [4, 80])\n",
    "    \n",
    "    if np.mean(Cxy) > 0.8:\n",
    "        continue\n",
    "    \n",
    "    if burst_a in cortex_ngb and burst_b in striatum_ngb:\n",
    "        Cxy_arr_ngb.append(Cxy)\n",
    "        mean_coherence_ngb.append(np.mean(Cxy))\n",
    "        bursts_a_ngb.append(a)\n",
    "        bursts_b_ngb.append(b)\n",
    "    elif burst_a in cortex_sb and burst_b in striatum_sb:\n",
    "        Cxy_arr_sb.append(Cxy)\n",
    "        mean_coherence_sb.append(np.mean(Cxy))\n",
    "        bursts_a_sb.append(a)\n",
    "        bursts_b_sb.append(b)\n",
    "\n",
    "for i in range(1000):\n",
    "    if (i+1) % 10 == 0:\n",
    "        print('Shuffle iteration', i+1)\n",
    "    \n",
    "    burst_pairs_shuffled_ngb = np.array([np.random.permutation(bursts_a_ngb), np.random.permutation(bursts_b_ngb)], dtype=object).T\n",
    "    burst_pairs_shuffled_sb = np.array([np.random.permutation(bursts_a_sb), np.random.permutation(bursts_b_sb)], dtype=object).T\n",
    "\n",
    "    Cxy_shuffled_inner_ngb = []\n",
    "    Cxy_shuffled_inner_sb = []\n",
    "    \n",
    "    for burst_pair in burst_pairs_shuffled_ngb:\n",
    "        a, b = burst_pair\n",
    "        f, Cxy = signal.coherence(a, b, fs=SAMPLING_RATE, nperseg=window, noverlap=overlap)\n",
    "        f, Cxy = get_psd_in_range((f, Cxy), [4, 80])\n",
    "        \n",
    "        Cxy_shuffled_inner_ngb.append(Cxy)\n",
    "    for burst_pair in burst_pairs_shuffled_sb:\n",
    "        a, b = burst_pair\n",
    "        f, Cxy = signal.coherence(a, b, fs=SAMPLING_RATE, nperseg=window, noverlap=overlap)\n",
    "        f, Cxy = get_psd_in_range((f, Cxy), [4, 80])\n",
    "        \n",
    "        Cxy_shuffled_inner_sb.append(Cxy)\n",
    "    \n",
    "    Cxy_shuffled_inner_ngb_mean = np.mean(Cxy_shuffled_inner_ngb, axis=0)\n",
    "    Cxy_shuffled_inner_sb_mean = np.mean(Cxy_shuffled_inner_sb, axis=0)\n",
    "    \n",
    "    Cxy_arr_shuffled_ngb.append(Cxy_shuffled_inner_ngb_mean)\n",
    "    Cxy_arr_shuffled_sb.append(Cxy_shuffled_inner_sb_mean)\n",
    "    \n",
    "    mean_coherence_shuffled_ngb.append(np.mean(Cxy_shuffled_inner_ngb_mean))\n",
    "    mean_coherence_shuffled_sb.append(np.mean(Cxy_shuffled_inner_sb_mean))\n",
    "    \n",
    "# Bar plot\n",
    "y = [\n",
    "    np.mean(mean_coherence_ngb),\n",
    "    np.mean(mean_coherence_sb)\n",
    "]\n",
    "yerr = [\n",
    "    np.std(mean_coherence_ngb)/len(mean_coherence_ngb)**0.5,\n",
    "    np.std(mean_coherence_sb)/len(mean_coherence_sb)**0.5\n",
    "]\n",
    "colors = [\n",
    "    \"tab:blue\"\n",
    "    \"tab:red\"\n",
    "]\n",
    "bars = plt.bar([\"NGB\", \"SB\"], y, yerr=yerr, capsize=5)\n",
    "for idx, bar_i in enumerate(bars):\n",
    "    bar_i.set_color(colors[idx])\n",
    "plt.ylabel('Mean coherence')\n",
    "format_plot(plt.gca(), legend=False)\n",
    "plt.show()\n",
    "print('ngb versus sb', stats.ttest_ind(mean_coherence_ngb, mean_coherence_sb, equal_var=False))\n",
    "print('ngb', stats.ttest_ind(mean_coherence_ngb, mean_coherence_shuffled_ngb, equal_var=False))\n",
    "print('sb', stats.ttest_ind(mean_coherence_sb, mean_coherence_shuffled_sb, equal_var=False))\n",
    "\n",
    "# Coherence plot \n",
    "mean_ngb = np.mean(Cxy_arr_ngb, axis=0)\n",
    "stderr_ngb = np.std(Cxy_arr_ngb, axis=0) / len(Cxy_arr_ngb)**0.5\n",
    "mean_sb = np.mean(Cxy_arr_sb, axis=0)\n",
    "stderr_sb = np.std(Cxy_arr_sb, axis=0) / len(Cxy_arr_ngb)**0.5\n",
    "\n",
    "# Coherence plot NGB\n",
    "plt.plot(f, mean_ngb, label='NGB', c='tab:blue')\n",
    "plt.fill_between(f, mean_ngb+stderr_ngb, mean_ngb-stderr_ngb, alpha=0.5, facecolor='tab:blue')\n",
    "shuffled_95_percentile_ngb = np.percentile(Cxy_arr_shuffled_ngb, 95, axis=0)\n",
    "plt.plot(f, shuffled_95_percentile_ngb, '--', label='Shuffled NGB', c='tab:blue')\n",
    "plt.xlabel('Frequency (Hz)')\n",
    "plt.ylabel('Coherence')\n",
    "plt.legend()\n",
    "format_plot(plt.gca())\n",
    "plt.show()\n",
    "\n",
    "# Coherence plot SB\n",
    "plt.plot(f, mean_sb, label='SB', c='tab:red')\n",
    "plt.fill_between(f, mean_sb+stderr_sb, mean_sb-stderr_sb, alpha=0.5, facecolor='tab:red')\n",
    "shuffled_95_percentile_sb = np.percentile(Cxy_arr_shuffled_sb, 95, axis=0)\n",
    "plt.plot(f, shuffled_95_percentile_sb, '--', label='Shuffled SB', c='tab:red')\n",
    "plt.xlabel('Frequency (Hz)')\n",
    "plt.ylabel('Coherence')\n",
    "plt.legend()\n",
    "format_plot(plt.gca())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lag analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cross-spectral coherence for burst pairs    \n",
    "window_size = 0.5\n",
    "window = int(SAMPLING_RATE*window_size)\n",
    "overlap = int(SAMPLING_RATE*window_size*0)\n",
    "\n",
    "xpos_arr = None\n",
    "cross_corr_arr_test = []\n",
    "peaks_arr_test = []\n",
    "\n",
    "cross_corr_arr_control = []\n",
    "peaks_arr_control = []\n",
    "\n",
    "bursts_a = []\n",
    "bursts_b = []\n",
    "\n",
    "test_bands = [] # 'Test' bands where coherence is maximal\n",
    "test_band_label = ''\n",
    "control_bands = [] # 'Control' bands where coherence is low, as a comparison\n",
    "control_band_label = ''\n",
    "\n",
    "events = np.concatenate([cooccuring_events['Ctx-CP']])\n",
    "\n",
    "for burst_pair_idx, burst_pair in enumerate(events):\n",
    "    burst_a, burst_b = burst_pair\n",
    "    a, b, ticks = align_overlapping_bursts (burst_a, burst_b)\n",
    "    \n",
    "    if (burst_pair_idx+1) % 10 == 0:\n",
    "        print( 'Burst pair {}/{}'.format(burst_pair_idx+1, len(events) ))\n",
    "\n",
    "    if not (burst_a in cortex_ngb and burst_b in striatum_ngb):\n",
    "        continue\n",
    "        \n",
    "    if len(a)/SAMPLING_RATE < 0.5:\n",
    "        continue\n",
    "        \n",
    "    a = butter_bandpass_filter(a, 4, 80)\n",
    "    b = butter_bandpass_filter(b, 4, 80)\n",
    "    \n",
    "    bursts_a.append(a)\n",
    "    bursts_b.append(b)\n",
    "\n",
    "    f, Cxy = signal.coherence(a, b, fs=SAMPLING_RATE, nperseg=window, noverlap=overlap)\n",
    "    f, Cxy = get_psd_in_range((f, Cxy), [4, 80])\n",
    "    \n",
    "    if np.mean(Cxy) > 0.8:\n",
    "        continue\n",
    "    \n",
    "    xpos, cross_corr = get_lags(a, b, test_bands, prewhiten=True)\n",
    "    xpos_arr = xpos\n",
    "    cross_corr_arr_test.append(cross_corr)\n",
    "    \n",
    "    peaks_arr_test.append(xpos[np.argmax(cross_corr)])\n",
    "    \n",
    "    xpos, cross_corr = get_lags(a, b, control_bands, prewhiten=True)\n",
    "    xpos_arr = xpos\n",
    "    cross_corr_arr_control.append(cross_corr)\n",
    "    peaks_arr_control.append(xpos[np.argmax(cross_corr)])\n",
    "\n",
    "means_test = np.mean(cross_corr_arr_test, axis=0)\n",
    "stderrs_test = np.std(cross_corr_arr_test, axis=0) / len(cross_corr_arr_test)**0.5\n",
    "means_control = np.mean(cross_corr_arr_control, axis=0)\n",
    "stderrs_control = np.std(cross_corr_arr_control, axis=0) / len(cross_corr_arr_control)**0.5\n",
    "\n",
    "plt.plot(xpos_arr, means_test, label=test_band_label, c='tab:red')\n",
    "plt.fill_between(xpos_arr, means_test-stderrs_test, means_test+stderrs_test, alpha=0.25, facecolor='tab:red')\n",
    "plt.plot(xpos_arr, means_control, label=control_band_label, c='tab:blue')\n",
    "plt.fill_between(xpos_arr, means_control-stderrs_control, means_control+stderrs_control, alpha=0.25, facecolor='tab:blue')\n",
    "plt.ylabel('Squared cross-correlation')\n",
    "plt.xlabel('Lag (ms)')\n",
    "plt.legend()\n",
    "format_plot(plt.gca())\n",
    "plt.show()\n",
    "\n",
    "means_test = np.mean(peaks_arr_test, axis=0)\n",
    "stderrs_test = np.std(peaks_arr_test, axis=0) / len(peaks_arr_test)**0.5\n",
    "means_control = np.mean(peaks_arr_control, axis=0)\n",
    "stderrs_control = np.std(peaks_arr_control, axis=0) / len(peaks_arr_control)**0.5\n",
    "\n",
    "bars = plt.bar([0, 1], [means_test, means_control], yerr=[stderrs_test, stderrs_control], capsize=5, facecolor='dimgray')\n",
    "bars[0].set_color('tab:red')\n",
    "bars[1].set_color('tab:blue')\n",
    "plt.xticks([0, 1], [test_band_label, control_band_label])\n",
    "plt.ylabel('Lag (ms)')\n",
    "format_plot(plt.gca(), legend=False)\n",
    "plt.show()\n",
    "\n",
    "print('5-15 Hz', stats.ttest_1samp(peaks_arr_test, popmean=0))\n",
    "print('25-35 Hz', stats.ttest_1samp(peaks_arr_control, popmean=0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
